{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sacred-value",
   "metadata": {},
   "source": [
    "# Kernel Methods: Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-sympathy",
   "metadata": {},
   "source": [
    "Julia Linhart, Roman Castagn√©, Louis Bouvier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-wedding",
   "metadata": {},
   "source": [
    "Preliminary functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "renewable-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(ids, labels, filename):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        - ids: list of ids, should be an increasing list of integers\n",
    "        - labels: list of corresponding labels, either 0 or 1\n",
    "        - file: string containing the name that should be given to the submission file    \n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"Id\": ids, \"Bound\": labels})\n",
    "    df.to_csv(filename, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-simple",
   "metadata": {},
   "source": [
    "# I) Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "meaningful-telescope",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "middle-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data' # 'machine-learning-with-kernel-methods-2021'\n",
    "\n",
    "X_train_1 = pd.read_csv(f'{data_folder}/Xtr2_mat100.csv', sep = ' ', index_col=False, header=None)\n",
    "y_train_1 = pd.read_csv(f'{data_folder}/Ytr2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "center-fossil",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Bound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4999.500000</td>\n",
       "      <td>0.498500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>577.494589</td>\n",
       "      <td>0.500123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4499.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4999.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5499.250000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5999.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id        Bound\n",
       "count  2000.000000  2000.000000\n",
       "mean   4999.500000     0.498500\n",
       "std     577.494589     0.500123\n",
       "min    4000.000000     0.000000\n",
       "25%    4499.750000     0.000000\n",
       "50%    4999.500000     0.000000\n",
       "75%    5499.250000     1.000000\n",
       "max    5999.000000     1.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "greatest-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_1 = np.array(y_train_1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "consolidated-characteristic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.010565</td>\n",
       "      <td>0.010201</td>\n",
       "      <td>0.010375</td>\n",
       "      <td>0.011587</td>\n",
       "      <td>0.011609</td>\n",
       "      <td>0.010707</td>\n",
       "      <td>0.009359</td>\n",
       "      <td>0.011957</td>\n",
       "      <td>0.009571</td>\n",
       "      <td>0.010582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007951</td>\n",
       "      <td>0.009457</td>\n",
       "      <td>0.008554</td>\n",
       "      <td>0.009283</td>\n",
       "      <td>0.008261</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.011141</td>\n",
       "      <td>0.009777</td>\n",
       "      <td>0.008217</td>\n",
       "      <td>0.008565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.012278</td>\n",
       "      <td>0.010723</td>\n",
       "      <td>0.011467</td>\n",
       "      <td>0.011453</td>\n",
       "      <td>0.012182</td>\n",
       "      <td>0.010478</td>\n",
       "      <td>0.009789</td>\n",
       "      <td>0.012444</td>\n",
       "      <td>0.013805</td>\n",
       "      <td>0.013652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009605</td>\n",
       "      <td>0.009701</td>\n",
       "      <td>0.009350</td>\n",
       "      <td>0.009741</td>\n",
       "      <td>0.012341</td>\n",
       "      <td>0.010338</td>\n",
       "      <td>0.010863</td>\n",
       "      <td>0.010402</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>0.009283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.076087</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.184783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.076087</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows √ó 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean      0.010565     0.010201     0.010375     0.011587     0.011609   \n",
       "std       0.012278     0.010723     0.011467     0.011453     0.012182   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.010870     0.010870     0.010870     0.010870     0.010870   \n",
       "75%       0.010870     0.021739     0.021739     0.021739     0.021739   \n",
       "max       0.086957     0.065217     0.097826     0.065217     0.065217   \n",
       "\n",
       "                5            6            7            8            9   ...  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000  ...   \n",
       "mean      0.010707     0.009359     0.011957     0.009571     0.010582  ...   \n",
       "std       0.010478     0.009789     0.012444     0.013805     0.013652  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "50%       0.010870     0.010870     0.010870     0.000000     0.010870  ...   \n",
       "75%       0.021739     0.010870     0.021739     0.010870     0.021739  ...   \n",
       "max       0.054348     0.054348     0.076087     0.097826     0.184783  ...   \n",
       "\n",
       "                90           91           92           93           94  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean      0.007951     0.009457     0.008554     0.009283     0.008261   \n",
       "std       0.009605     0.009701     0.009350     0.009741     0.012341   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.010870     0.010870     0.010870     0.010870     0.000000   \n",
       "75%       0.010870     0.010870     0.010870     0.010870     0.010870   \n",
       "max       0.054348     0.065217     0.054348     0.054348     0.086957   \n",
       "\n",
       "                95           96           97           98           99  \n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000  \n",
       "mean      0.009614     0.011141     0.009777     0.008217     0.008565  \n",
       "std       0.010338     0.010863     0.010402     0.009709     0.009283  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.010870     0.010870     0.010870     0.010870     0.010870  \n",
       "75%       0.010870     0.021739     0.010870     0.010870     0.010870  \n",
       "max       0.065217     0.076087     0.065217     0.065217     0.043478  \n",
       "\n",
       "[8 rows x 100 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "federal-shelf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train_1 = np.array(X_train_1)\n",
    "print(X_train_1.shape)\n",
    "X_train_1 = (X_train_1 - X_train_1.mean(axis=0))/X_train_1.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "impressive-essence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-contribution",
   "metadata": {},
   "source": [
    "# II) First linear models of the mat100 input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-mambo",
   "metadata": {},
   "source": [
    "## A) Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fresh-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(z):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    - z (any size): an array-like element\n",
    "    ouput:\n",
    "    - the element-wize application of the sigmo√Ød function on z\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dietary-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X,y,w,b):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: dxN): the points we want to classify\n",
    "    - y (size: 1xN): the values of the classes\n",
    "    - w (size: dx1): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    output:\n",
    "    - the opposite of the log-likelihood of the Logistic Regression model computed with respect to\n",
    "    the points (X,y) and the parameters w,b\n",
    "    \"\"\"\n",
    "    X_tilde = np.vstack((X,np.ones(X.shape[1])))\n",
    "    w_tilde = np.vstack((w,b))\n",
    "    return -np.sum(y*np.log(g(w_tilde.T@X_tilde)) + (1-y)*np.log(1-g(w_tilde.T@X_tilde)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "injured-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad(X,y,w,b):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: dxN): the points we want to classify\n",
    "    - y (size: 1xN): the values of the classes\n",
    "    - w (size: dx1): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    output:\n",
    "    - the gradient of the loss of the Logistic Regression model computed \n",
    "    with respect to (w,b) = w_tilde having the points (X,y) \n",
    "    \"\"\"\n",
    "    X_tilde = np.vstack((X,np.ones(X.shape[1])))\n",
    "    w_tilde = np.vstack((w,b))    \n",
    "    return -X_tilde@(y-g(w_tilde.T@X_tilde)).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "enclosed-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hess(X,y,w,b):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: dxN): the points we want to classify\n",
    "    - y (size: 1xN): the values of the classes\n",
    "    - w (size: dx1): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    output:\n",
    "    - the hessian of the loss of the Logistic Regression model computed \n",
    "    with respect to (w,b) = w_tilde having the points (X,y) \n",
    "    \"\"\"\n",
    "    X_tilde = np.vstack((X,np.ones(X.shape[1])))\n",
    "    w_tilde = np.vstack((w,b))    \n",
    "    temp = (g(w_tilde.T@X_tilde)*(g(w_tilde.T@X_tilde)-1)).reshape(-1,)\n",
    "    return -X_tilde@np.diag(temp)@X_tilde.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "frequent-vitamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(X,y,w,b,delta,grad,alpha=0.1,beta=0.7):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: dxN): the points we want to classify\n",
    "    - y (size: 1xN): the values of the classes\n",
    "    - w (size: dx1): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    - delta (size n): direction of the search\n",
    "    - grad (size n): value of the gradient at point (w,b)\n",
    "    - alpha: factor of the slope of the line in the backtracking line search\n",
    "    - beta: factor of reduction of the step length\n",
    "    \n",
    "    outputs:\n",
    "    - t: the step length for the Newton step on the objective function\n",
    "    computed with backtracking line search towards delta\"\"\"\n",
    "        \n",
    "    t = 1\n",
    "    while(compute_loss(X,y,w+t*delta[:-1],b+t*delta[-1])>\n",
    "            compute_loss(X,y,w,b) + alpha*t*grad.T@delta):\n",
    "        t = beta*t\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "robust-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton(X,y,w0,b0,eps=pow(10,-1)):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: dxN): the points we want to classify\n",
    "    - y (size: 1xN): the values of the classes\n",
    "    - w0 (size: dx1): the initial weights of the affine mapping of x\n",
    "    - b0 (size: 1x1): the initial constant of the affine mapping of x\n",
    "    output:\n",
    "    - the paramer vector w_tilde_hat = (w_hat, b_hat) which maximizes the log-likelihood of \n",
    "    the sample (X,y) in the Logistic Regression model (or minimizes the loss)\n",
    "    - the cached values of the loss evaluated along training\n",
    "    \"\"\"\n",
    "    w_,b_ = w0,b0\n",
    "    grad = compute_grad(X,y,w0,b0)\n",
    "    hess = compute_hess(X,y,w0,b0)\n",
    "    print(hess.shape)\n",
    "#     inv_hess = np.linalg.inv(compute_hess(X,y,w0,b0))\n",
    "    inv_hess, _, _, _ = np.linalg.lstsq(hess, np.eye(hess.shape[0]))\n",
    "    dec_2 = grad.T@inv_hess@grad\n",
    "    Loss_hist = [compute_loss(X,y,w0,b0)]\n",
    "    while dec_2/2>eps:## condition on the Newton decrement\n",
    "        grad = compute_grad(X,y,w_,b_)\n",
    "        hess = compute_hess(X,y,w_,b_)\n",
    "#         inv_hess = np.linalg.inv(compute_hess(X,y,w_,b_))\n",
    "        inv_hess, _, _, _ = np.linalg.lstsq(hess, np.eye(hess.shape[0]))\n",
    "        dec_2 = grad.T@inv_hess@grad\n",
    "        delta = -inv_hess@grad\n",
    "        t_bt = backtracking(X,y,w_,b_,delta,grad)\n",
    "        w_ = w_ + t_bt*delta[:-1]\n",
    "        b_ = b_ + t_bt*delta[-1]\n",
    "        Loss_hist.append(compute_loss(X,y,w_,b_))\n",
    "    return w_, b_, Loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "provincial-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_LogReg(x,w,b):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - x (size dx1): a point in R^d\n",
    "    - w (size: dx1): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    output:\n",
    "     - the predicted class for the associated y given the\n",
    "    Logistic Regression parameters\n",
    "    \"\"\"    \n",
    "    return (w.T@x+b>0).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "threatened-classification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1900)\n",
      "(100, 100)\n",
      "(1, 1900)\n",
      "(1, 100)\n"
     ]
    }
   ],
   "source": [
    "dim = 100\n",
    "Nb_samples = 2000\n",
    "prop_test = 0.05\n",
    "Train_indices = np.random.choice(a=Nb_samples, size=int((1-prop_test)*Nb_samples), replace=False)\n",
    "mask_train = np.zeros(Nb_samples, dtype=bool)\n",
    "np.put_along_axis(arr=mask_train, indices=Train_indices, values=True, axis=0)\n",
    "\n",
    "X_tr = X_train_1[mask_train,:].T\n",
    "X_te = X_train_1[np.logical_not(mask_train),:].T\n",
    "print(X_tr.shape)\n",
    "print(X_te.shape)\n",
    "y_tr = y_train_1[mask_train].reshape(1,-1)\n",
    "y_te = y_train_1[np.logical_not(mask_train)].reshape(1,-1)\n",
    "print(y_tr.shape)\n",
    "print(y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "invisible-outside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 101)\n",
      "Misclassification error:\n",
      "On train set: 26.53%\n",
      "On test set: 32.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louisbouvier/Documents/MVA-MPRO/MVA/S2/Learning/Kernel Methods/challenge/KM_env/lib/python3.7/site-packages/ipykernel_launcher.py:18: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "/Users/louisbouvier/Documents/MVA-MPRO/MVA/S2/Learning/Kernel Methods/challenge/KM_env/lib/python3.7/site-packages/ipykernel_launcher.py:25: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    }
   ],
   "source": [
    "## compute the corresponding MLE on train set\n",
    "w0, b0 = np.random.randn(100,1)*0.07, np.zeros((1,1))## we initialize parameters\n",
    "w_hat, b_hat, _ = Newton(X_tr,y_tr,w0,b0)\n",
    "    \n",
    "## assess the convergence of the Newton Method\n",
    "#print(\"w_hat = {}\".format(w_hat))\n",
    "#print(\"b_hat = {}\".format(b_hat))\n",
    "    \n",
    "## predict on the two sets\n",
    "y_predicted_train = predict_LogReg(X_tr,w_hat,b_hat)## prediction on train set\n",
    "mis_class_err_train = np.sum(y_predicted_train!=y_tr)/y_tr.shape[1]\n",
    "y_predicted_test = predict_LogReg(X_te,w_hat,b_hat)## prediction on train set\n",
    "mis_class_err_test = np.sum(y_predicted_test!=y_te)/y_te.shape[1]\n",
    "print(\"Misclassification error:\")\n",
    "print(\"On train set: {:.2f}%\".format(100*mis_class_err_train))\n",
    "print(\"On test set: {:.2f}%\".format(100*mis_class_err_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "retained-carroll",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.481\n",
      "(101, 101)\n",
      "Misclassification error on set 0:\n",
      "On train set: 35.21%\n",
      "On test set: 38.00%\n",
      "0.459\n",
      "0.5005\n",
      "(101, 101)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louisbouvier/Documents/MVA-MPRO/MVA/S2/Learning/Kernel Methods/challenge/KM_env/lib/python3.7/site-packages/ipykernel_launcher.py:18: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "/Users/louisbouvier/Documents/MVA-MPRO/MVA/S2/Learning/Kernel Methods/challenge/KM_env/lib/python3.7/site-packages/ipykernel_launcher.py:25: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification error on set 1:\n",
      "On train set: 36.89%\n",
      "On test set: 36.00%\n",
      "0.499\n",
      "0.4985\n",
      "(101, 101)\n",
      "Misclassification error on set 2:\n",
      "On train set: 28.11%\n",
      "On test set: 21.00%\n",
      "0.497\n"
     ]
    }
   ],
   "source": [
    "dim = 100\n",
    "Nb_samples = 2000\n",
    "prop_test = 0.05\n",
    "\n",
    "all_y_eval = []\n",
    "\n",
    "np.random.seed(1)\n",
    "for name in [0, 1, 2]:\n",
    "    X = pd.read_csv(f'{data_folder}/Xtr{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
    "    y = pd.read_csv(f'{data_folder}/Ytr{name}.csv')\n",
    "    y = y[\"Bound\"].to_numpy()\n",
    "    \n",
    "    mean, std = X.mean(axis=0), X.std(axis=0)\n",
    "    \n",
    "    X = (X - mean)/std\n",
    "\n",
    "    tr_indices = np.random.choice(Nb_samples, size=int((1-prop_test)*Nb_samples), replace=False)\n",
    "    te_indices = [idx for idx in range(Nb_samples) if idx not in tr_indices]\n",
    "#     mask_train = np.zeros(Nb_samples, dtype=bool)\n",
    "#     mask_train[tr_indices] = 1\n",
    "#     np.put_along_axis(arr=mask_train, indices=tr_indices, values=True, axis=0)\n",
    "\n",
    "    X_tr = X[tr_indices].T\n",
    "    X_te = X[te_indices].T\n",
    "    \n",
    "    assert X_tr.shape[1] + X_te.shape[1] == X.shape[0]\n",
    "#     print(X_tr.shape)\n",
    "#     print(X_te.shape)\n",
    "    y_tr = y[tr_indices].reshape(1,-1)\n",
    "    y_te = y[te_indices].reshape(1,-1)\n",
    "    assert y_tr.shape[1] + y_te.shape[1] == y.shape[0]\n",
    "#     print(y_tr.shape)\n",
    "#     print(y_te.shape)\n",
    "    print(y.sum() / y.shape[0])\n",
    "    \n",
    "    ## compute the corresponding MLE on train set\n",
    "    w0, b0 = np.random.randn(100,1)*0.07, np.zeros((1,1)) # we initialize parameters\n",
    "    w_hat, b_hat, _ = Newton(X_tr,y_tr,w0,b0)\n",
    "\n",
    "    ## assess the convergence of the Newton Method\n",
    "#     print(\"w_hat = {}\".format(w_hat))\n",
    "#     print(\"b_hat = {}\".format(b_hat))\n",
    "\n",
    "    ## predict on the two sets\n",
    "    y_predicted_train = predict_LogReg(X_tr,w_hat,b_hat) # prediction on train set\n",
    "    mis_class_err_train = np.sum(y_predicted_train!=y_tr)/y_tr.shape[1]\n",
    "    y_predicted_test = predict_LogReg(X_te,w_hat,b_hat) # prediction on test set\n",
    "    mis_class_err_test = np.sum(y_predicted_test!=y_te)/y_te.shape[1]\n",
    "    print(f\"Misclassification error on set {name}:\")\n",
    "    print(\"On train set: {:.2f}%\".format(100*mis_class_err_train))\n",
    "    print(\"On test set: {:.2f}%\".format(100*mis_class_err_test))\n",
    "    \n",
    "    # predict on the new set\n",
    "    X_eval = pd.read_csv(f'{data_folder}/Xte{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
    "    X_eval = (X_eval - mean)/std\n",
    "    y_eval = predict_LogReg(X_eval.T,w_hat,b_hat)\n",
    "    all_y_eval.append(y_eval)\n",
    "    print(y_eval.sum() / y_eval.shape[1])\n",
    "    \n",
    "all_y_eval = np.hstack(all_y_eval).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "finnish-phoenix",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ids = np.arange(all_y_eval.shape[0])\n",
    "filename = \"results/submission_log_reg.csv\"\n",
    "\n",
    "write_csv(ids, all_y_eval, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-butter",
   "metadata": {},
   "source": [
    "## B) Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "magnetic-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_RR_MLE(X,y,lamb):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: dxN): the points we want to classify\n",
    "    - y (size: 1xN): the values of the classes\n",
    "    outputs:\n",
    "    - the value of MLE estimation (w_hat, b_hat) in the Linear regression model\n",
    "    \"\"\"\n",
    "    X_tilde = np.vstack((X,np.ones(X.shape[1])))\n",
    "    temp = np.linalg.inv(X_tilde@X_tilde.T +lamb*X.shape[1]*np.eye(1+X.shape[0]))@X_tilde@y.T\n",
    "    return temp[:-1], temp[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "noble-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_RR(x,w,b):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - x (size dx1): a point in R^d\n",
    "    - w (size: dx1): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    output:\n",
    "     - the predicted class for the associated y given the\n",
    "    Linear Regression parameters\n",
    "    \"\"\"    \n",
    "    return (w.T@x+b>1/2).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "hydraulic-trace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.481\n",
      "Misclassification error on set 0:\n",
      "On train set: 36.00%\n",
      "On test set: 42.00%\n",
      "0.364\n",
      "0.5005\n",
      "Misclassification error on set 1:\n",
      "On train set: 36.68%\n",
      "On test set: 44.00%\n",
      "0.381\n",
      "0.4985\n",
      "Misclassification error on set 2:\n",
      "On train set: 27.68%\n",
      "On test set: 37.00%\n",
      "0.423\n"
     ]
    }
   ],
   "source": [
    "dim = 100\n",
    "Nb_samples = 2000\n",
    "prop_test = 0.05\n",
    "\n",
    "all_y_eval = []\n",
    "\n",
    "np.random.seed(1)\n",
    "for name in [0, 1, 2]:\n",
    "    X = pd.read_csv(f'{data_folder}/Xtr{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
    "    y = pd.read_csv(f'{data_folder}/Ytr{name}.csv')\n",
    "    y = y[\"Bound\"].to_numpy()\n",
    "    \n",
    "    mean, std = X.mean(axis=0), X.std(axis=0)\n",
    "    \n",
    "    X = (X - mean)/std\n",
    "\n",
    "    tr_indices = np.random.choice(Nb_samples, size=int((1-prop_test)*Nb_samples), replace=False)\n",
    "    te_indices = [idx for idx in range(Nb_samples) if idx not in tr_indices]\n",
    "\n",
    "    X_tr = X[tr_indices].T\n",
    "    X_te = X[te_indices].T\n",
    "    \n",
    "    assert X_tr.shape[1] + X_te.shape[1] == X.shape[0]\n",
    "#     print(X_tr.shape)\n",
    "#     print(X_te.shape)\n",
    "    y_tr = y[tr_indices].reshape(1,-1)\n",
    "    y_te = y[te_indices].reshape(1,-1)\n",
    "    assert y_tr.shape[1] + y_te.shape[1] == y.shape[0]\n",
    "#     print(y_tr.shape)\n",
    "#     print(y_te.shape)\n",
    "    print(y.sum() / y.shape[0])\n",
    "    \n",
    "    ## compute the corresponding MLE on train set\n",
    "    w_hat, b_hat = compute_RR_MLE(X_tr,y_tr,0.1)\n",
    "\n",
    "    ## assess the convergence of the Newton Method\n",
    "#     print(\"w_hat = {}\".format(w_hat))\n",
    "#     print(\"b_hat = {}\".format(b_hat))\n",
    "\n",
    "    ## predict on the two sets\n",
    "    y_predicted_train = predict_RR(X_tr,w_hat,b_hat) # prediction on train set\n",
    "    mis_class_err_train = np.sum(y_predicted_train!=y_tr)/y_tr.shape[1]\n",
    "    y_predicted_test = predict_RR(X_te,w_hat,b_hat) # prediction on test set\n",
    "    mis_class_err_test = np.sum(y_predicted_test!=y_te)/y_te.shape[1]\n",
    "    print(f\"Misclassification error on set {name}:\")\n",
    "    print(\"On train set: {:.2f}%\".format(100*mis_class_err_train))\n",
    "    print(\"On test set: {:.2f}%\".format(100*mis_class_err_test))\n",
    "    \n",
    "    # predict on the new set\n",
    "    X_eval = pd.read_csv(f'{data_folder}/Xte{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
    "    X_eval = (X_eval - mean)/std\n",
    "    y_eval = predict_RR(X_eval.T,w_hat,b_hat)\n",
    "    all_y_eval.append(y_eval)\n",
    "    print(y_eval.sum() / y_eval.shape[1])\n",
    "    \n",
    "all_y_eval = np.hstack(all_y_eval).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-triangle",
   "metadata": {},
   "source": [
    "# III) Kernel baselines "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-multiple",
   "metadata": {},
   "source": [
    "## A) Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "excess-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian_kernel(X1, X2, sig):\n",
    "    \"\"\"inputs:\n",
    "    - X1 (size dxN1): a set of points\n",
    "    - X2 (size dxN2): another one  \n",
    "    - sig (float): the std of the kernel\n",
    "    ouput:\n",
    "    - the associated (N1xN2) Gaussian kernel\n",
    "    \"\"\"\n",
    "    return np.exp(-distance_matrix(X1.T,X2.T)/(2*sig**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "caroline-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_KRR_MLE(X, y, lamb, sig=10):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: dxN): the points we want to classify\n",
    "    - y (size: 1xN): the values of the classes\n",
    "    outputs:\n",
    "    - the value of MLE estimation (w_hat, b_hat) in the Linear regression model\n",
    "    \"\"\"\n",
    "    K = Gaussian_kernel(X, X, sig=sig)\n",
    "    alpha = np.linalg.inv(K+lamb*X.shape[1]*np.eye(X.shape[1]))@y.T\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "portable-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_KRR(X_tr, X_te, alpha, sig=10):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X_tr (size dx1): a point in R^d\n",
    "    - w (size: dx1): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    output:\n",
    "     - the predicted class for the associated y given the\n",
    "    Linear Regression parameters\n",
    "    \"\"\"    \n",
    "    K_te_tr = Gaussian_kernel(X_tr, X_te, sig=sig)\n",
    "    return 2*(alpha.T@K_te_tr>0).astype(\"int\")-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "rational-wallet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma = 1.2\n",
      "-0.038\n",
      "Misclassification error on set 0:\n",
      "On train set: 6.06%\n",
      "On test set: 50.25%\n",
      "0.99\n",
      "Sigma = 1.2\n",
      "0.001\n",
      "Misclassification error on set 1:\n",
      "On train set: 0.25%\n",
      "On test set: 46.75%\n",
      "0.994\n",
      "Sigma = 1.2\n",
      "-0.003\n",
      "Misclassification error on set 2:\n",
      "On train set: 2.50%\n",
      "On test set: 36.50%\n",
      "0.973\n"
     ]
    }
   ],
   "source": [
    "dim = 100\n",
    "Nb_samples = 2000\n",
    "prop_test = 0.2\n",
    "lamb = 0.5\n",
    "sigma = 1.2\n",
    "\n",
    "all_y_eval = []\n",
    "\n",
    "np.random.seed(1)\n",
    "for name in [0, 1, 2]:\n",
    "    X = pd.read_csv(f'{data_folder}/Xtr{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
    "    y = pd.read_csv(f'{data_folder}/Ytr{name}.csv')\n",
    "    y = y[\"Bound\"].to_numpy()\n",
    "    y[y==0] = -1\n",
    "    \n",
    "    mean, std = X.mean(axis=0), X.std(axis=0)\n",
    "    print(\"Sigma = {}\".format(sigma))\n",
    "    X = (X - mean)/std\n",
    "\n",
    "    tr_indices = np.random.choice(Nb_samples, size=int((1-prop_test)*Nb_samples), replace=False)\n",
    "    te_indices = [idx for idx in range(Nb_samples) if idx not in tr_indices]\n",
    "\n",
    "    X_tr = X[tr_indices].T\n",
    "    X_te = X[te_indices].T\n",
    "    \n",
    "    assert X_tr.shape[1] + X_te.shape[1] == X.shape[0]\n",
    "#     print(X_tr.shape)\n",
    "#     print(X_te.shape)\n",
    "    y_tr = y[tr_indices].reshape(1,-1)\n",
    "    y_te = y[te_indices].reshape(1,-1)\n",
    "    assert y_tr.shape[1] + y_te.shape[1] == y.shape[0]\n",
    "#     print(y_tr.shape)\n",
    "#     print(y_te.shape)\n",
    "    print(y.sum() / y.shape[0])\n",
    "    \n",
    "    ## compute the corresponding MLE on train set\n",
    "    alpha_hat = compute_KRR_MLE(X_tr,y_tr,lamb=lamb,sig=sigma)\n",
    "\n",
    "    ## assess the convergence of the Newton Method\n",
    "#     print(\"w_hat = {}\".format(w_hat))\n",
    "#     print(\"b_hat = {}\".format(b_hat))\n",
    "\n",
    "    ## predict on the two sets\n",
    "    y_predicted_train = predict_KRR(X_tr, X_tr, alpha_hat, sig=sigma) # prediction on train set\n",
    "    mis_class_err_train = np.sum(y_predicted_train!=y_tr)/y_tr.shape[1]\n",
    "    y_predicted_test = predict_KRR(X_tr, X_te, alpha_hat, sig=sigma) # prediction on test set\n",
    "    mis_class_err_test = np.sum(y_predicted_test!=y_te)/y_te.shape[1]\n",
    "    print(f\"Misclassification error on set {name}:\")\n",
    "    print(\"On train set: {:.2f}%\".format(100*mis_class_err_train))\n",
    "    print(\"On test set: {:.2f}%\".format(100*mis_class_err_test))\n",
    "    \n",
    "    # predict on the new set\n",
    "    X_eval = pd.read_csv(f'{data_folder}/Xte{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
    "    X_eval = (X_eval - mean)/std\n",
    "    y_eval = predict_LogReg(X_eval.T,w_hat,b_hat)\n",
    "    all_y_eval.append(y_eval)\n",
    "    print(y_eval.sum() / y_eval.shape[1])\n",
    "    \n",
    "all_y_eval = np.hstack(all_y_eval).reshape(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
