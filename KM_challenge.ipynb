{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "chief-nickname",
   "metadata": {},
   "source": [
    "# Kernel Methods: Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-discretion",
   "metadata": {},
   "source": [
    "Julia Linhart, Roman Castagné, Louis Bouvier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-airfare",
   "metadata": {},
   "source": [
    "# I) Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "unlimited-passport",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "secret-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = pd.read_csv('machine-learning-with-kernel-methods-2021/Xtr2_mat100.csv', sep = ' ', index_col=False, header=None)\n",
    "y_train_1 = pd.read_csv('machine-learning-with-kernel-methods-2021/Ytr2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "lightweight-playback",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Bound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4999.500000</td>\n",
       "      <td>0.498500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>577.494589</td>\n",
       "      <td>0.500123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4499.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4999.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5499.250000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5999.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id        Bound\n",
       "count  2000.000000  2000.000000\n",
       "mean   4999.500000     0.498500\n",
       "std     577.494589     0.500123\n",
       "min    4000.000000     0.000000\n",
       "25%    4499.750000     0.000000\n",
       "50%    4999.500000     0.000000\n",
       "75%    5499.250000     1.000000\n",
       "max    5999.000000     1.000000"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "difficult-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_1 = np.array(y_train_1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "herbal-means",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.010565</td>\n",
       "      <td>0.010201</td>\n",
       "      <td>0.010375</td>\n",
       "      <td>0.011587</td>\n",
       "      <td>0.011609</td>\n",
       "      <td>0.010707</td>\n",
       "      <td>0.009359</td>\n",
       "      <td>0.011957</td>\n",
       "      <td>0.009571</td>\n",
       "      <td>0.010582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007951</td>\n",
       "      <td>0.009457</td>\n",
       "      <td>0.008554</td>\n",
       "      <td>0.009283</td>\n",
       "      <td>0.008261</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.011141</td>\n",
       "      <td>0.009777</td>\n",
       "      <td>0.008217</td>\n",
       "      <td>0.008565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.012278</td>\n",
       "      <td>0.010723</td>\n",
       "      <td>0.011467</td>\n",
       "      <td>0.011453</td>\n",
       "      <td>0.012182</td>\n",
       "      <td>0.010478</td>\n",
       "      <td>0.009789</td>\n",
       "      <td>0.012444</td>\n",
       "      <td>0.013805</td>\n",
       "      <td>0.013652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009605</td>\n",
       "      <td>0.009701</td>\n",
       "      <td>0.009350</td>\n",
       "      <td>0.009741</td>\n",
       "      <td>0.012341</td>\n",
       "      <td>0.010338</td>\n",
       "      <td>0.010863</td>\n",
       "      <td>0.010402</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>0.009283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.076087</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.184783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.076087</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean      0.010565     0.010201     0.010375     0.011587     0.011609   \n",
       "std       0.012278     0.010723     0.011467     0.011453     0.012182   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.010870     0.010870     0.010870     0.010870     0.010870   \n",
       "75%       0.010870     0.021739     0.021739     0.021739     0.021739   \n",
       "max       0.086957     0.065217     0.097826     0.065217     0.065217   \n",
       "\n",
       "                5            6            7            8            9   ...  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000  ...   \n",
       "mean      0.010707     0.009359     0.011957     0.009571     0.010582  ...   \n",
       "std       0.010478     0.009789     0.012444     0.013805     0.013652  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "50%       0.010870     0.010870     0.010870     0.000000     0.010870  ...   \n",
       "75%       0.021739     0.010870     0.021739     0.010870     0.021739  ...   \n",
       "max       0.054348     0.054348     0.076087     0.097826     0.184783  ...   \n",
       "\n",
       "                90           91           92           93           94  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean      0.007951     0.009457     0.008554     0.009283     0.008261   \n",
       "std       0.009605     0.009701     0.009350     0.009741     0.012341   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.010870     0.010870     0.010870     0.010870     0.000000   \n",
       "75%       0.010870     0.010870     0.010870     0.010870     0.010870   \n",
       "max       0.054348     0.065217     0.054348     0.054348     0.086957   \n",
       "\n",
       "                95           96           97           98           99  \n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000  \n",
       "mean      0.009614     0.011141     0.009777     0.008217     0.008565  \n",
       "std       0.010338     0.010863     0.010402     0.009709     0.009283  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.010870     0.010870     0.010870     0.010870     0.010870  \n",
       "75%       0.010870     0.021739     0.010870     0.010870     0.010870  \n",
       "max       0.065217     0.076087     0.065217     0.065217     0.043478  \n",
       "\n",
       "[8 rows x 100 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "adapted-palestine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train_1 = np.array(X_train_1)\n",
    "print(X_train_1.shape)\n",
    "X_train_1 = (X_train_1 - X_train_1.mean(axis=0))/X_train_1.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "center-stomach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-gilbert",
   "metadata": {},
   "source": [
    "# II) First models of the mat100 input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-thompson",
   "metadata": {},
   "source": [
    "## A) Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "generic-intensity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(z):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    - z (any size): an array-like element\n",
    "    ouput:\n",
    "    - the element-wize application of the sigmoïd function on z\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "beautiful-render",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X,y,w,b):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: dxN): the points we want to classify\n",
    "    - y (size: 1xN): the values of the classes\n",
    "    - w (size: dx1): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    output:\n",
    "    - the opposite of the log-likelihood of the Logistic Regression model computed with respect to\n",
    "    the points (X,y) and the parameters w,b\n",
    "    \"\"\"\n",
    "    X_tilde = np.vstack((X,np.ones(X.shape[1])))\n",
    "    w_tilde = np.vstack((w,b))\n",
    "    return -np.sum(y*np.log(g(w_tilde.T@X_tilde)) + (1-y)*np.log(1-g(w_tilde.T@X_tilde)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "descending-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad(X,y,w,b):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: dxN): the points we want to classify\n",
    "    - y (size: 1xN): the values of the classes\n",
    "    - w (size: dx1): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    output:\n",
    "    - the gradient of the loss of the Logistic Regression model computed \n",
    "    with respect to (w,b) = w_tilde having the points (X,y) \n",
    "    \"\"\"\n",
    "    X_tilde = np.vstack((X,np.ones(X.shape[1])))\n",
    "    w_tilde = np.vstack((w,b))    \n",
    "    return -X_tilde@(y-g(w_tilde.T@X_tilde)).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "superb-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hess(X,y,w,b):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: dxN): the points we want to classify\n",
    "    - y (size: 1xN): the values of the classes\n",
    "    - w (size: dx1): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    output:\n",
    "    - the hessian of the loss of the Logistic Regression model computed \n",
    "    with respect to (w,b) = w_tilde having the points (X,y) \n",
    "    \"\"\"\n",
    "    X_tilde = np.vstack((X,np.ones(X.shape[1])))\n",
    "    w_tilde = np.vstack((w,b))    \n",
    "    temp = (g(w_tilde.T@X_tilde)*(g(w_tilde.T@X_tilde)-1)).reshape(-1,)\n",
    "    return -X_tilde@np.diag(temp)@X_tilde.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "greenhouse-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(X,y,w,b,delta,grad,alpha=0.1,beta=0.7):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: dxN): the points we want to classify\n",
    "    - y (size: 1xN): the values of the classes\n",
    "    - w (size: dx1): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    - delta (size n): direction of the search\n",
    "    - grad (size n): value of the gradient at point (w,b)\n",
    "    - alpha: factor of the slope of the line in the backtracking line search\n",
    "    - beta: factor of reduction of the step length\n",
    "    \n",
    "    outputs:\n",
    "    - t: the step length for the Newton step on the objective function\n",
    "    computed with backtracking line search towards delta\"\"\"\n",
    "        \n",
    "    t = 1\n",
    "    while(compute_loss(X,y,w+t*delta[:-1],b+t*delta[-1])>\n",
    "            compute_loss(X,y,w,b) + alpha*t*grad.T@delta):\n",
    "        t = beta*t\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "governing-architect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton(X,y,w0,b0,eps=pow(10,-1)):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: dxN): the points we want to classify\n",
    "    - y (size: 1xN): the values of the classes\n",
    "    - w0 (size: dx1): the initial weights of the affine mapping of x\n",
    "    - b0 (size: 1x1): the initial constant of the affine mapping of x\n",
    "    output:\n",
    "    - the paramer vector w_tilde_hat = (w_hat, b_hat) which maximizes the log-likelihood of \n",
    "    the sample (X,y) in the Logistic Regression model (or minimizes the loss)\n",
    "    - the cached values of the loss evaluated along training\n",
    "    \"\"\"\n",
    "    w_,b_ = w0,b0\n",
    "    grad = compute_grad(X,y,w0,b0)\n",
    "    inv_hess = np.linalg.inv(compute_hess(X,y,w0,b0))\n",
    "    dec_2 = grad.T@inv_hess@grad\n",
    "    Loss_hist = [compute_loss(X,y,w0,b0)]\n",
    "    while dec_2/2>eps:## condition on the Newton decrement\n",
    "        grad = compute_grad(X,y,w_,b_)\n",
    "        inv_hess = np.linalg.inv(compute_hess(X,y,w_,b_))\n",
    "        dec_2 = grad.T@inv_hess@grad\n",
    "        delta = -inv_hess@grad\n",
    "        t_bt = backtracking(X,y,w_,b_,delta,grad)\n",
    "        w_ = w_ + t_bt*delta[:-1]\n",
    "        b_ = b_ + t_bt*delta[-1]\n",
    "        Loss_hist.append(compute_loss(X,y,w_,b_))\n",
    "    return w_, b_, Loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "lonely-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_LogReg(x,w,b):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - x (size dx1): a point in R^d\n",
    "    - w (size: dx1): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    output:\n",
    "     - the predicted class for the associated y given the\n",
    "    Logistic Regression parameters\n",
    "    \"\"\"    \n",
    "    return (w.T@x+b>0).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "protected-aging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1800)\n",
      "(100, 200)\n",
      "(1, 1800)\n",
      "(1, 200)\n"
     ]
    }
   ],
   "source": [
    "dim = 100\n",
    "Nb_samples = 2000\n",
    "prop_test = 0.1\n",
    "Train_indices = np.random.choice(a=Nb_samples, size=int((1-prop_test)*Nb_samples), replace=False)\n",
    "mask_train = np.zeros(Nb_samples, dtype=bool)\n",
    "np.put_along_axis(arr=mask_train, indices=Train_indices, values=True, axis=0)\n",
    "\n",
    "X_tr = X_train_1[mask_train,:].T\n",
    "X_te = X_train_1[np.logical_not(mask_train),:].T\n",
    "print(X_tr.shape)\n",
    "print(X_te.shape)\n",
    "y_tr = y_train_1[mask_train].reshape(1,-1)\n",
    "y_te = y_train_1[np.logical_not(mask_train)].reshape(1,-1)\n",
    "print(y_tr.shape)\n",
    "print(y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "alternative-device",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification error:\n",
      "On train set: 26.50%\n",
      "On test set: 32.00%\n"
     ]
    }
   ],
   "source": [
    "## compute the corresponding MLE on train set\n",
    "w0, b0 = np.random.randn(100,1)*0.07, np.zeros((1,1))## we initialize parameters\n",
    "w_hat, b_hat, _ = Newton(X_tr,y_tr,w0,b0)\n",
    "    \n",
    "## assess the convergence of the Newton Method\n",
    "#print(\"w_hat = {}\".format(w_hat))\n",
    "#print(\"b_hat = {}\".format(b_hat))\n",
    "    \n",
    "## predict on the two sets\n",
    "y_predicted_train = predict_LogReg(X_tr,w_hat,b_hat)## prediction on train set\n",
    "mis_class_err_train = np.sum(y_predicted_train!=y_tr)/y_tr.shape[1]\n",
    "y_predicted_test = predict_LogReg(X_te,w_hat,b_hat)## prediction on train set\n",
    "mis_class_err_test = np.sum(y_predicted_test!=y_te)/y_te.shape[1]\n",
    "print(\"Misclassification error:\")\n",
    "print(\"On train set: {:.2f}%\".format(100*mis_class_err_train))\n",
    "print(\"On test set: {:.2f}%\".format(100*mis_class_err_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KM_env",
   "language": "python",
   "name": "km_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
