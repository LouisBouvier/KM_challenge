{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "KM_challenge.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pz797qSo1Wd9"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/MVA/KKML')"
      ],
      "id": "Pz797qSo1Wd9",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqUQTmZ20az9"
      },
      "source": [
        "# Kernel Methods: Challenge"
      ],
      "id": "RqUQTmZ20az9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAISFXMk0a0D"
      },
      "source": [
        "Julia Linhart, Roman Castagn√©, Louis Bouvier"
      ],
      "id": "RAISFXMk0a0D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXbC5eRC0a0D"
      },
      "source": [
        "Preliminary functions:"
      ],
      "id": "NXbC5eRC0a0D"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvAm-YyB0a0E"
      },
      "source": [
        "def write_csv(ids, labels, filename):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "        - ids: list of ids, should be an increasing list of integers\n",
        "        - labels: list of corresponding labels, either 0 or 1\n",
        "        - file: string containing the name that should be given to the submission file    \n",
        "    \"\"\"\n",
        "    df = pd.DataFrame({\"Id\": ids, \"Bound\": labels})\n",
        "    df[\"Bound\"] = df[\"Bound\"].replace([-1], 0)\n",
        "    df.to_csv(filename, sep=',', index=False)"
      ],
      "id": "ZvAm-YyB0a0E",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiFKdzlB0a0E"
      },
      "source": [
        "# I) Preprocessing"
      ],
      "id": "jiFKdzlB0a0E"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKNsQc940a0F"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from functools import partial\n",
        "from scipy.spatial import distance_matrix\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import cvxpy as cp\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "id": "ZKNsQc940a0F",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrXWzksi0a0F"
      },
      "source": [
        "data_folder = 'data' # 'machine-learning-with-kernel-methods-2021'\n",
        "\n",
        "X_train_1 = pd.read_csv(f'{data_folder}/Xtr2_mat100.csv', sep = ' ', index_col=False, header=None)\n",
        "y_train_1 = pd.read_csv(f'{data_folder}/Ytr2.csv')"
      ],
      "id": "XrXWzksi0a0F",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "8SY4m9HA0a0F",
        "outputId": "bb2b1fd8-36f1-4ec4-97ee-40dc6aeac607"
      },
      "source": [
        "y_train_1.describe()"
      ],
      "id": "8SY4m9HA0a0F",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Bound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>4999.500000</td>\n",
              "      <td>0.498500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>577.494589</td>\n",
              "      <td>0.500123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>4000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>4499.750000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4999.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>5499.250000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>5999.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Id        Bound\n",
              "count  2000.000000  2000.000000\n",
              "mean   4999.500000     0.498500\n",
              "std     577.494589     0.500123\n",
              "min    4000.000000     0.000000\n",
              "25%    4499.750000     0.000000\n",
              "50%    4999.500000     0.000000\n",
              "75%    5499.250000     1.000000\n",
              "max    5999.000000     1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcoPE7dE0a0H"
      },
      "source": [
        "y_train_1 = np.array(y_train_1)[:,1]"
      ],
      "id": "JcoPE7dE0a0H",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "bgo-XP0u0a0I",
        "outputId": "4fa3e853-5782-4b65-a4a0-eb8906fd7df5"
      },
      "source": [
        "X_train_1.describe()"
      ],
      "id": "bgo-XP0u0a0I",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.010565</td>\n",
              "      <td>0.010201</td>\n",
              "      <td>0.010375</td>\n",
              "      <td>0.011587</td>\n",
              "      <td>0.011609</td>\n",
              "      <td>0.010707</td>\n",
              "      <td>0.009359</td>\n",
              "      <td>0.011957</td>\n",
              "      <td>0.009571</td>\n",
              "      <td>0.010582</td>\n",
              "      <td>0.009424</td>\n",
              "      <td>0.009793</td>\n",
              "      <td>0.012848</td>\n",
              "      <td>0.012092</td>\n",
              "      <td>0.011196</td>\n",
              "      <td>0.010364</td>\n",
              "      <td>0.009875</td>\n",
              "      <td>0.010962</td>\n",
              "      <td>0.010185</td>\n",
              "      <td>0.008342</td>\n",
              "      <td>0.010734</td>\n",
              "      <td>0.010038</td>\n",
              "      <td>0.011554</td>\n",
              "      <td>0.008995</td>\n",
              "      <td>0.010283</td>\n",
              "      <td>0.008647</td>\n",
              "      <td>0.008886</td>\n",
              "      <td>0.008826</td>\n",
              "      <td>0.007821</td>\n",
              "      <td>0.009761</td>\n",
              "      <td>0.008533</td>\n",
              "      <td>0.011864</td>\n",
              "      <td>0.009299</td>\n",
              "      <td>0.010641</td>\n",
              "      <td>0.009560</td>\n",
              "      <td>0.008929</td>\n",
              "      <td>0.010217</td>\n",
              "      <td>0.009641</td>\n",
              "      <td>0.009880</td>\n",
              "      <td>0.010038</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009511</td>\n",
              "      <td>0.010614</td>\n",
              "      <td>0.011957</td>\n",
              "      <td>0.009641</td>\n",
              "      <td>0.011772</td>\n",
              "      <td>0.009500</td>\n",
              "      <td>0.008783</td>\n",
              "      <td>0.010005</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.009147</td>\n",
              "      <td>0.013565</td>\n",
              "      <td>0.010587</td>\n",
              "      <td>0.009793</td>\n",
              "      <td>0.010908</td>\n",
              "      <td>0.009500</td>\n",
              "      <td>0.009772</td>\n",
              "      <td>0.009103</td>\n",
              "      <td>0.010147</td>\n",
              "      <td>0.008587</td>\n",
              "      <td>0.010538</td>\n",
              "      <td>0.010897</td>\n",
              "      <td>0.008913</td>\n",
              "      <td>0.008630</td>\n",
              "      <td>0.008380</td>\n",
              "      <td>0.009016</td>\n",
              "      <td>0.011478</td>\n",
              "      <td>0.008832</td>\n",
              "      <td>0.009989</td>\n",
              "      <td>0.010587</td>\n",
              "      <td>0.008625</td>\n",
              "      <td>0.007951</td>\n",
              "      <td>0.009457</td>\n",
              "      <td>0.008554</td>\n",
              "      <td>0.009283</td>\n",
              "      <td>0.008261</td>\n",
              "      <td>0.009614</td>\n",
              "      <td>0.011141</td>\n",
              "      <td>0.009777</td>\n",
              "      <td>0.008217</td>\n",
              "      <td>0.008565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.012278</td>\n",
              "      <td>0.010723</td>\n",
              "      <td>0.011467</td>\n",
              "      <td>0.011453</td>\n",
              "      <td>0.012182</td>\n",
              "      <td>0.010478</td>\n",
              "      <td>0.009789</td>\n",
              "      <td>0.012444</td>\n",
              "      <td>0.013805</td>\n",
              "      <td>0.013652</td>\n",
              "      <td>0.012934</td>\n",
              "      <td>0.011163</td>\n",
              "      <td>0.027178</td>\n",
              "      <td>0.018160</td>\n",
              "      <td>0.011200</td>\n",
              "      <td>0.010356</td>\n",
              "      <td>0.010089</td>\n",
              "      <td>0.019951</td>\n",
              "      <td>0.010631</td>\n",
              "      <td>0.009920</td>\n",
              "      <td>0.011238</td>\n",
              "      <td>0.010962</td>\n",
              "      <td>0.011475</td>\n",
              "      <td>0.009723</td>\n",
              "      <td>0.010922</td>\n",
              "      <td>0.009933</td>\n",
              "      <td>0.009622</td>\n",
              "      <td>0.009861</td>\n",
              "      <td>0.010099</td>\n",
              "      <td>0.010628</td>\n",
              "      <td>0.009945</td>\n",
              "      <td>0.010829</td>\n",
              "      <td>0.010358</td>\n",
              "      <td>0.010460</td>\n",
              "      <td>0.011039</td>\n",
              "      <td>0.009612</td>\n",
              "      <td>0.010705</td>\n",
              "      <td>0.012258</td>\n",
              "      <td>0.020208</td>\n",
              "      <td>0.011266</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010436</td>\n",
              "      <td>0.011172</td>\n",
              "      <td>0.012915</td>\n",
              "      <td>0.010912</td>\n",
              "      <td>0.011305</td>\n",
              "      <td>0.016977</td>\n",
              "      <td>0.014644</td>\n",
              "      <td>0.012108</td>\n",
              "      <td>0.011800</td>\n",
              "      <td>0.009647</td>\n",
              "      <td>0.011868</td>\n",
              "      <td>0.011752</td>\n",
              "      <td>0.013102</td>\n",
              "      <td>0.010237</td>\n",
              "      <td>0.009652</td>\n",
              "      <td>0.009687</td>\n",
              "      <td>0.011871</td>\n",
              "      <td>0.010457</td>\n",
              "      <td>0.012348</td>\n",
              "      <td>0.011010</td>\n",
              "      <td>0.011005</td>\n",
              "      <td>0.010695</td>\n",
              "      <td>0.009248</td>\n",
              "      <td>0.010494</td>\n",
              "      <td>0.009279</td>\n",
              "      <td>0.011204</td>\n",
              "      <td>0.010571</td>\n",
              "      <td>0.015973</td>\n",
              "      <td>0.009745</td>\n",
              "      <td>0.011904</td>\n",
              "      <td>0.009605</td>\n",
              "      <td>0.009701</td>\n",
              "      <td>0.009350</td>\n",
              "      <td>0.009741</td>\n",
              "      <td>0.012341</td>\n",
              "      <td>0.010338</td>\n",
              "      <td>0.010863</td>\n",
              "      <td>0.010402</td>\n",
              "      <td>0.009709</td>\n",
              "      <td>0.009283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.010870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.097826</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.054348</td>\n",
              "      <td>0.054348</td>\n",
              "      <td>0.076087</td>\n",
              "      <td>0.097826</td>\n",
              "      <td>0.184783</td>\n",
              "      <td>0.108696</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.239130</td>\n",
              "      <td>0.141304</td>\n",
              "      <td>0.076087</td>\n",
              "      <td>0.054348</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.141304</td>\n",
              "      <td>0.054348</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.076087</td>\n",
              "      <td>0.108696</td>\n",
              "      <td>0.076087</td>\n",
              "      <td>0.054348</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.054348</td>\n",
              "      <td>0.054348</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.097826</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.119565</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>0.054348</td>\n",
              "      <td>0.141304</td>\n",
              "      <td>0.206522</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>...</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.119565</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.119565</td>\n",
              "      <td>0.239130</td>\n",
              "      <td>0.076087</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.076087</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>0.054348</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.054348</td>\n",
              "      <td>0.097826</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.076087</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.108696</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.097826</td>\n",
              "      <td>0.054348</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.054348</td>\n",
              "      <td>0.054348</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.076087</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.043478</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows √ó 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                0            1   ...           98           99\n",
              "count  2000.000000  2000.000000  ...  2000.000000  2000.000000\n",
              "mean      0.010565     0.010201  ...     0.008217     0.008565\n",
              "std       0.012278     0.010723  ...     0.009709     0.009283\n",
              "min       0.000000     0.000000  ...     0.000000     0.000000\n",
              "25%       0.000000     0.000000  ...     0.000000     0.000000\n",
              "50%       0.010870     0.010870  ...     0.010870     0.010870\n",
              "75%       0.010870     0.021739  ...     0.010870     0.010870\n",
              "max       0.086957     0.065217  ...     0.065217     0.043478\n",
              "\n",
              "[8 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJU9wcKY0a0I",
        "outputId": "0f85cc84-d6c1-4259-a9fd-8eae7acdf52e"
      },
      "source": [
        "X_train_1 = np.array(X_train_1)\n",
        "print(X_train_1.shape)\n",
        "X_train_1 = (X_train_1 - X_train_1.mean(axis=0))/X_train_1.std(axis=0)"
      ],
      "id": "ZJU9wcKY0a0I",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8G8Z3xuG0a0J",
        "outputId": "cb0bb08c-a8d5-4eda-f723-847c647f955b"
      },
      "source": [
        "print(y_train_1.shape)"
      ],
      "id": "8G8Z3xuG0a0J",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJFmTps10a0J"
      },
      "source": [
        "# II) First linear models of the mat100 input"
      ],
      "id": "hJFmTps10a0J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovYDI_gw0a0J"
      },
      "source": [
        "## A) Logistic regression"
      ],
      "id": "ovYDI_gw0a0J"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E7S-54J0a0K"
      },
      "source": [
        "def g(z):\n",
        "    \"\"\"\n",
        "    input:\n",
        "    - z (any size): an array-like element\n",
        "    ouput:\n",
        "    - the element-wize application of the sigmo√Ød function on z\n",
        "    \"\"\"\n",
        "    return 1/(1+np.exp(-z))"
      ],
      "id": "_E7S-54J0a0K",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktw_MldY0a0K"
      },
      "source": [
        "def compute_loss(X,y,w,b):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "    - X (size: Nxd): the points we want to classify\n",
        "    - y (size: Nx1): the values of the classes\n",
        "    - w (size: 1xd): the weights of the affine mapping of x\n",
        "    - b (size: 1x1): the constant of the affine mapping of x\n",
        "    output:\n",
        "    - the opposite of the log-likelihood of the Logistic Regression model computed with respect to\n",
        "    the points (X,y) and the parameters w,b\n",
        "    \"\"\"\n",
        "    X_tilde = np.hstack([X, np.ones((X.shape[0], 1))])\n",
        "    w_tilde = np.hstack((w,b))\n",
        "    return -np.sum(y * np.log(g(w_tilde@X_tilde.T)) + (1-y) * np.log(1-g(w_tilde@X_tilde.T)), axis=1)"
      ],
      "id": "ktw_MldY0a0K",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohAlip1w0a0K"
      },
      "source": [
        "def compute_grad(X,y,w,b):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "    - X (size: Nxd): the points we want to classify\n",
        "    - y (size: Nx1): the values of the classes\n",
        "    - w (size: 1xd): the weights of the affine mapping of x\n",
        "    - b (size: 1x1): the constant of the affine mapping of x\n",
        "    output:\n",
        "    - the gradient of the loss of the Logistic Regression model computed \n",
        "    with respect to (w,b) = w_tilde having the points (X,y) \n",
        "    \"\"\"\n",
        "    X_tilde = np.hstack([X, np.ones((X.shape[0], 1))])\n",
        "    w_tilde = np.hstack((w,b))\n",
        "    return -X_tilde.T @ (y - g(w_tilde@X_tilde.T).reshape(-1,))"
      ],
      "id": "ohAlip1w0a0K",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhijbHZk0a0L"
      },
      "source": [
        "def compute_hess(X,y,w,b):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "    - X (size: Nxd): the points we want to classify\n",
        "    - y (size: Nx1): the values of the classes\n",
        "    - w (size: 1xd): the weights of the affine mapping of x\n",
        "    - b (size: 1x1): the constant of the affine mapping of x\n",
        "    output:\n",
        "    - the hessian of the loss of the Logistic Regression model computed \n",
        "    with respect to (w,b) = w_tilde having the points (X,y) \n",
        "    \"\"\"\n",
        "    X_tilde = np.hstack([X, np.ones((X.shape[0], 1))])\n",
        "    w_tilde = np.hstack((w,b))    \n",
        "    temp = (g(w_tilde @ X_tilde.T) * (g(w_tilde @ X_tilde.T) - 1)).reshape(-1,)\n",
        "    return -X_tilde.T @ np.diag(temp) @ X_tilde"
      ],
      "id": "PhijbHZk0a0L",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB6KETwa0a0L"
      },
      "source": [
        "def backtracking(X,y,w,b,delta,grad,alpha=0.1,beta=0.7):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "    - X (size: Nxd): the points we want to classify\n",
        "    - y (size: Nx1): the values of the classes\n",
        "    - w (size: 1xd): the weights of the affine mapping of x\n",
        "    - b (size: 1x1): the constant of the affine mapping of x\n",
        "    - delta (size n): direction of the search\n",
        "    - grad (size n): value of the gradient at point (w,b)\n",
        "    - alpha: factor of the slope of the line in the backtracking line search\n",
        "    - beta: factor of reduction of the step length\n",
        "    \n",
        "    outputs:\n",
        "    - t: the step length for the Newton step on the objective function\n",
        "    computed with backtracking line search towards delta\"\"\"\n",
        "        \n",
        "    t = 1\n",
        "    while(compute_loss(X, y, w+t*delta[:-1], b+t*delta[-1])>\n",
        "            compute_loss(X,y,w,b) + alpha*t*grad.T @ delta):\n",
        "        t = beta*t\n",
        "    return t"
      ],
      "id": "CB6KETwa0a0L",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-jkPwuu0a0M"
      },
      "source": [
        "def Newton(X, y, w0, b0, eps=pow(10,-1)):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "    - X (size: Nxd): the points we want to classify\n",
        "    - y (size: Nx1): the values of the classes\n",
        "    - w0 (size: 1xd): the initial weights of the affine mapping of x\n",
        "    - b0 (size: 1x1): the initial constant of the affine mapping of x\n",
        "    output:\n",
        "    - the paramer vector w_tilde_hat = (w_hat, b_hat) which maximizes the log-likelihood of \n",
        "    the sample (X,y) in the Logistic Regression model (or minimizes the loss)\n",
        "    - the cached values of the loss evaluated along training\n",
        "    \"\"\"\n",
        "    w_, b_ = w0, b0\n",
        "    grad = compute_grad(X, y, w0, b0)\n",
        "    hess = compute_hess(X, y, w0, b0)\n",
        "    \n",
        "#     inv_hess = np.linalg.inv(compute_hess(X,y,w0,b0))\n",
        "    inv_hess, _, _, _ = np.linalg.lstsq(hess, np.eye(hess.shape[0]))\n",
        "    dec_2 = grad.T@inv_hess@grad\n",
        "    Loss_hist = [compute_loss(X,y,w0,b0)]\n",
        "    while dec_2/2 > eps: # condition on the Newton decrement\n",
        "        grad = compute_grad(X,y,w_,b_)\n",
        "        hess = compute_hess(X,y,w_,b_)\n",
        "        \n",
        "#         inv_hess = np.linalg.inv(compute_hess(X,y,w_,b_))\n",
        "        inv_hess, _, _, _ = np.linalg.lstsq(hess, np.eye(hess.shape[0]))\n",
        "        dec_2 = grad.T@inv_hess@grad\n",
        "        delta = - inv_hess@grad\n",
        "        t_bt = backtracking(X, y, w_, b_, delta, grad)\n",
        "        w_ = w_ + t_bt*delta[:-1]\n",
        "        b_ = b_ + t_bt*delta[-1]\n",
        "        Loss_hist.append(compute_loss(X,y,w_,b_))\n",
        "    return w_, b_, Loss_hist"
      ],
      "id": "c-jkPwuu0a0M",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_kXI2MU0a0M"
      },
      "source": [
        "def predict_LogReg(x,w,b):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "    - x (size 1xd): a point in R^d\n",
        "    - w (size: 1xd): the weights of the affine mapping of x\n",
        "    - b (size: 1x1): the constant of the affine mapping of x\n",
        "    output:\n",
        "     - the predicted class for the associated y given the\n",
        "    Logistic Regression parameters\n",
        "    \"\"\"    \n",
        "    return (w.T@x + b > 0).astype(\"int\")"
      ],
      "id": "V_kXI2MU0a0M",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG8epkzE0a0N"
      },
      "source": [
        "class LogisticRegressor(BaseEstimator, ClassifierMixin):\n",
        "    \n",
        "    def __init__(self, lamb=1.):\n",
        "        \"\"\"\n",
        "        This class implements methods for fitting and predicting with a LogesticRegression for classification \n",
        "        inputs:\n",
        "        - lamb : the regularisation parameter\n",
        "        \"\"\"\n",
        "        self.lamb = lamb\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        inputs:\n",
        "        - X (size: Nxd): the points we want to classify\n",
        "        - y (size: Nx1): the values of the classes\n",
        "        outputs:\n",
        "        - the value of MLE estimation (w_hat, b_hat) in the Linear regression model\n",
        "        \"\"\"\n",
        "        w0, b0 = np.random.randn(1, 100)*0.07, np.zeros((1,1))\n",
        "        self.w_, self.b_, _ = Newton(X, y, w0, b0)\n",
        "        \n",
        "        return self\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        inputs:\n",
        "        - X (size Nxd): a point in R^d\n",
        "        - w (size: 1xd): the weights of the affine mapping of x\n",
        "        - b (size: 1x1): the constant of the affine mapping of x\n",
        "        output:\n",
        "         - the predicted class for the associated y given the\n",
        "        Linear Regression parameters\n",
        "        \"\"\"    \n",
        "        return (self.w_@X.T + self.b_ > 1/2).astype(\"int\")\n",
        "        \n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        inputs:\n",
        "        - X (size Nxd): the points in R^d we want to classify\n",
        "        - y (size Nx1): the labels of the points\n",
        "        \"\"\"\n",
        "        y_pred = self.predict(X)\n",
        "        return np.sum(y_pred == y)/y.shape[0]"
      ],
      "id": "DG8epkzE0a0N",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuYAhP_70a0O",
        "outputId": "9799757c-a1f0-46b3-cb40-9a623ca6c35f"
      },
      "source": [
        "dim = 100\n",
        "Nb_samples = 2000\n",
        "prop_test = 0.05\n",
        "\n",
        "all_y_eval = []\n",
        "\n",
        "np.random.seed(1)\n",
        "for name in [0, 1, 2]:\n",
        "    X = pd.read_csv(f'{data_folder}/Xtr{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
        "    y = pd.read_csv(f'{data_folder}/Ytr{name}.csv')\n",
        "    y = y[\"Bound\"].to_numpy()\n",
        "    \n",
        "    mean, std = X.mean(axis=0), X.std(axis=0)\n",
        "    X = (X - mean)/std\n",
        "\n",
        "    tr_indices = np.random.choice(Nb_samples, size=int((1-prop_test)*Nb_samples), replace=False)\n",
        "    te_indices = [idx for idx in range(Nb_samples) if idx not in tr_indices]\n",
        "\n",
        "    X_tr = X[tr_indices]\n",
        "    X_te = X[te_indices]\n",
        "    \n",
        "    y_tr = y[tr_indices]\n",
        "    y_te = y[te_indices]\n",
        "    \n",
        "    assert X_tr.shape[0] + X_te.shape[0] == X.shape[0]\n",
        "    assert y_tr.shape[0] + y_te.shape[0] == y.shape[0]\n",
        "    \n",
        "    # Fitting\n",
        "    logreg = LogisticRegressor()\n",
        "    \n",
        "    logreg.fit(X_tr, y_tr)\n",
        "\n",
        "    \n",
        "    print(f\"Accuracy on train set {name}: {logreg.score(X_tr, y_tr):.2f}\")\n",
        "    print(f\"Accuracy on test set {name} : {logreg.score(X_te, y_te):.2f}\")   \n",
        "    \n",
        "    # Prediction on the new set\n",
        "    X_eval = pd.read_csv(f'{data_folder}/Xte{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
        "    X_eval = (X_eval - mean)/std\n",
        "    y_eval = logreg.predict(X_eval)\n",
        "    all_y_eval.append(y_eval)\n",
        "    \n",
        "all_y_eval = np.hstack(all_y_eval).reshape(-1)"
      ],
      "id": "xuYAhP_70a0O",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy on train set 0: 0.62\n",
            "Accuracy on test set 0 : 0.56\n",
            "Accuracy on train set 1: 0.60\n",
            "Accuracy on test set 1 : 0.59\n",
            "Accuracy on train set 2: 0.70\n",
            "Accuracy on test set 2 : 0.66\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "N-_1EYnY0a0O"
      },
      "source": [
        "ids = np.arange(all_y_eval.shape[0])\n",
        "filename = \"results/submission_log_reg.csv\"\n",
        "\n",
        "# write_csv(ids, all_y_eval, filename)"
      ],
      "id": "N-_1EYnY0a0O",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LPAM0yW0a0P"
      },
      "source": [
        "## B) Ridge regression"
      ],
      "id": "2LPAM0yW0a0P"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZYbRtxE0a0Q"
      },
      "source": [
        "def compute_RR_MLE(X,y,lamb):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "    - X (size: Nxd): the points we want to classify\n",
        "    - y (size: Nx1): the values of the classes\n",
        "    outputs:\n",
        "    - the value of MLE estimation (w_hat, b_hat) in the Linear regression model\n",
        "    \"\"\"\n",
        "    X_tilde = np.vstack((X,np.ones(X.shape[1])))\n",
        "    temp = np.linalg.inv(X_tilde@X_tilde.T + lamb*X.shape[1]*np.eye(1+X.shape[0]))@X_tilde@y.T\n",
        "    return temp[:-1], temp[-1]"
      ],
      "id": "wZYbRtxE0a0Q",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3923wu80a0Q"
      },
      "source": [
        "def predict_RR(x,w,b):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "    - x (size 1xd): a point in R^d\n",
        "    - w (size: 1xd): the weights of the affine mapping of x\n",
        "    - b (size: 1x1): the constant of the affine mapping of x\n",
        "    output:\n",
        "     - the predicted class for the associated y given the\n",
        "    Linear Regression parameters\n",
        "    \"\"\"    \n",
        "    return (w.T@x+b>1/2).astype(\"int\")"
      ],
      "id": "d3923wu80a0Q",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wux5BgTY0a0Q"
      },
      "source": [
        "class RidgeRegressor(BaseEstimator, ClassifierMixin):\n",
        "    \n",
        "    def __init__(self, lamb=1.):\n",
        "        \"\"\"\n",
        "        This class implements methods for fitting and predicting with a RidgeRegressor used for classification \n",
        "        (by thresholding the value regressed).\n",
        "        inputs:\n",
        "        - lamb : the regularisation parameter\n",
        "        \"\"\"\n",
        "        self.lamb = lamb\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        inputs:\n",
        "        - X (size: Nxd): the points we want to classify\n",
        "        - y (size: Nx1): the values of the classes\n",
        "        outputs:\n",
        "        - the value of MLE estimation (w_hat, b_hat) in the Linear regression model\n",
        "        \"\"\"\n",
        "        X_tilde = np.hstack((X, np.ones((X.shape[0], 1))))\n",
        "        temp = np.linalg.inv(X_tilde.T @ X_tilde + self.lamb * X.shape[0] * np.eye(X_tilde.shape[1])) @ (X_tilde.T @ y)\n",
        "        self.w_ = temp[:-1]\n",
        "        self.b_ = temp[-1]\n",
        "\n",
        "        return self\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        inputs:\n",
        "        - x (size Nxd): a point in R^d\n",
        "        - w (size: 1xd): the weights of the affine mapping of x\n",
        "        - b (size: 1x1): the constant of the affine mapping of x\n",
        "        output:\n",
        "         - the predicted class for the associated y given the\n",
        "        Linear Regression parameters\n",
        "        \"\"\"    \n",
        "        return (self.w_@X.T + self.b_ > 1/2).astype(\"int\")\n",
        "        \n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        inputs:\n",
        "        - X (size Nxd): the points in R^d we want to classify\n",
        "        - y (size Nx1): the labels of the points\n",
        "        \"\"\"\n",
        "        y_pred = self.predict(X)\n",
        "        return np.sum(y_pred == y)/y.shape[0]"
      ],
      "id": "wux5BgTY0a0Q",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK4wdATY0a0Q",
        "outputId": "93c497fb-859c-4d42-828c-dccdf6907f6c"
      },
      "source": [
        "dim = 100\n",
        "Nb_samples = 2000\n",
        "prop_test = 0.05\n",
        "lamb = 0.1\n",
        "\n",
        "all_y_eval = []\n",
        "\n",
        "np.random.seed(1)\n",
        "for name in [0, 1, 2]:\n",
        "    # Data processing\n",
        "    X = pd.read_csv(f'{data_folder}/Xtr{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
        "    y = pd.read_csv(f'{data_folder}/Ytr{name}.csv')\n",
        "    y = y[\"Bound\"].to_numpy()\n",
        "    \n",
        "    mean, std = X.mean(axis=0), X.std(axis=0)\n",
        "    X = (X - mean)/std\n",
        "\n",
        "    tr_indices = np.random.choice(Nb_samples, size=int((1-prop_test)*Nb_samples), replace=False)\n",
        "    te_indices = [idx for idx in range(Nb_samples) if idx not in tr_indices]\n",
        "\n",
        "    X_tr = X[tr_indices]\n",
        "    X_te = X[te_indices]\n",
        "    \n",
        "    y_tr = y[tr_indices]\n",
        "    y_te = y[te_indices]\n",
        "    \n",
        "    assert X_tr.shape[0] + X_te.shape[0] == X.shape[0]\n",
        "    assert y_tr.shape[0] + y_te.shape[0] == y.shape[0]\n",
        "    \n",
        "    # Fitting the classifier\n",
        "    params = {'lamb': np.linspace(0.001, 0.1, 20)}\n",
        "    rr = GridSearchCV(RidgeRegressor(), params)\n",
        "#     rr = RidgeRegressor(lamb=lamb)\n",
        "\n",
        "    rr.fit(X_tr, y_tr)\n",
        "    \n",
        "    print(rr.best_params_)\n",
        "    \n",
        "    print(f\"Accuracy on train set {name}: {rr.score(X_tr, y_tr):.2f}\")\n",
        "    print(f\"Accuracy on test set {name} : {rr.score(X_te, y_te):.2f}\")\n",
        "    \n",
        "    # Prediction on the new set\n",
        "    X_eval = pd.read_csv(f'{data_folder}/Xte{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
        "    X_eval = (X_eval - mean)/std\n",
        "    y_eval = rr.predict(X_eval)\n",
        "    all_y_eval.append(y_eval)\n",
        "    \n",
        "all_y_eval = np.hstack(all_y_eval).reshape(-1)"
      ],
      "id": "zK4wdATY0a0Q",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'lamb': 0.021842105263157895}\n",
            "Accuracy on train set 0: 0.65\n",
            "Accuracy on test set 0 : 0.60\n",
            "{'lamb': 0.001}\n",
            "Accuracy on train set 1: 0.64\n",
            "Accuracy on test set 1 : 0.57\n",
            "{'lamb': 0.04268421052631579}\n",
            "Accuracy on train set 2: 0.73\n",
            "Accuracy on test set 2 : 0.69\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtjBOLml0a0R"
      },
      "source": [
        "# III) Kernel baselines "
      ],
      "id": "BtjBOLml0a0R"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ie35LaC0a0R"
      },
      "source": [
        "## A) Kernels"
      ],
      "id": "2Ie35LaC0a0R"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eOe1mTP0a0R"
      },
      "source": [
        "def Gaussian_kernel(X1, X2, sig):\n",
        "    \"\"\"inputs:\n",
        "    - X1 (size N1xd): a set of points\n",
        "    - X2 (size N2xd): another one  \n",
        "    - sig (float): the std of the kernel\n",
        "    ouput:\n",
        "    - the associated (N1xN2) Gaussian kernel\n",
        "    \"\"\"\n",
        "    return np.exp(-distance_matrix(X1,X2)/(2*sig**2))"
      ],
      "id": "1eOe1mTP0a0R",
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rKFOspuz7IS"
      },
      "source": [
        "from itertools import product\n",
        "\n",
        "def spectrum(x,k):\n",
        "  l = len(x)\n",
        "  spectrum_x = np.array([x[i:(i + k)] for i in range(l - k + 1)])\n",
        "  return np.array(spectrum_x)\n",
        "\n",
        "def Spectrum_kernel(X1, X2, k):\n",
        "  \"\"\"inputs:\n",
        "    - X1 (size N1xd): a set of sequences\n",
        "    - X2 (size N2xd): another one  \n",
        "    - k (int): the length of the substrings \n",
        "    ouput:\n",
        "    - the associated (N1xN2) Spectrum kernel\n",
        "    \"\"\"\n",
        "  # substrings: all possible combinations of A,T,G,C of length k\n",
        "  A_k = [''.join(s) for s in product([\"A\", \"T\", \"G\", \"C\"], repeat=k)]\n",
        "\n",
        "  # nb of occurances of the elements of A_k in the k-spectrum of X1 (resp. X2)\n",
        "  phi_spect_X1 = np.array([[np.sum(spectrum(x,k)==u) for u in A_k] for x in X1])\n",
        "  phi_spect_X2 = np.array([[np.sum(spectrum(x,k)==u) for u in A_k] for x in X2])\n",
        "  \n",
        "  return phi_spect_X1 @ phi_spect_X2.T\n"
      ],
      "id": "9rKFOspuz7IS",
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBFBUDeE0a0R"
      },
      "source": [
        "## B) Algorithms\n",
        "### 1. Kernel Ridge Regression"
      ],
      "id": "OBFBUDeE0a0R"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mg97hlq0a0R"
      },
      "source": [
        "def compute_KRR_MLE(X, y, lamb, sig=10):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "    - X (size: N_trxd): the points of the training set\n",
        "    - y (size: N_trx1): the values of the classes\n",
        "    outputs:\n",
        "    - the value of MLE estimation (w_hat, b_hat) in the kernel ridge regression model\n",
        "    \"\"\"\n",
        "    K = Gaussian_kernel(X, X, sig=sig)\n",
        "    alpha = np.linalg.inv(K+lamb*X.shape[1]*np.eye(X.shape[1]))@y.T\n",
        "    return alpha"
      ],
      "id": "1Mg97hlq0a0R",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8xihAC20a0R"
      },
      "source": [
        "def predict_KRR(X_tr, X_te, alpha, sig=10):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "    - X_tr (size N_trxd): the points of the training set\n",
        "    - X_te (size N_texd): the points of the test set we want to classify\n",
        "    - w (size: 1xd): the weights of the affine mapping \n",
        "    - b (size: 1x1): the constant of the affine mapping\n",
        "    output:\n",
        "     - the predicted class for the associated y_te given the\n",
        "    Linear Regression parameters\n",
        "    \"\"\"    \n",
        "    K_te_tr = Gaussian_kernel(X_tr, X_te, sig=sig)\n",
        "    return 2*(alpha.T@K_te_tr>0).astype(\"int\")-1"
      ],
      "id": "G8xihAC20a0R",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHIjYIWA0a0S"
      },
      "source": [
        "class KernelRidgeRegressor(BaseEstimator, ClassifierMixin):\n",
        "    \n",
        "    def __init__(self, lamb=1., sigma=1., kernel='gaussian'):\n",
        "        \"\"\"\n",
        "        This class implements methods for fitting and predicting with a KernelRidgeRegressor used for classification \n",
        "        (by thresholding the value regressed). Any kernel can be used. \n",
        "        inputs:\n",
        "        - lamb : the regularisation parameter \n",
        "        - sigma : the parameter of the Gaussian kernel (if Gaussian kernel selected)\n",
        "        - kernel : the kernel we consider\n",
        "        \"\"\"\n",
        "        self.lamb = lamb\n",
        "        self.sigma = sigma\n",
        "        self.kernel = kernel\n",
        "        if self.kernel == 'gaussian':\n",
        "            self.kernel_ = partial(Gaussian_kernel, sig=sigma)\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Kernel {self.kernel} is not implemented yet\")\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        inputs:\n",
        "        - X (size: N_trxd): the points of the training set\n",
        "        - y (size: N_trx1): the values of the classes\n",
        "        \"\"\"\n",
        "        # We keep values of training in memory for prediction\n",
        "        self.X_tr_ = np.copy(X)\n",
        "        K = self.kernel_(X, X, sig=self.sigma)\n",
        "        self.alpha_ = np.linalg.inv(K+self.lamb*X.shape[0]*np.eye(X.shape[0]))@y\n",
        "        \n",
        "        return self\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        inputs:\n",
        "        - X (size N_texd): the points in R^d we want to classify\n",
        "        output:\n",
        "         - the predicted class for the associated y given the\n",
        "        Linear Regression parameters\n",
        "        \"\"\"\n",
        "        K_tr_te = self.kernel_(self.X_tr_, X, sig=self.sigma)\n",
        "        \n",
        "        return 2 * (self.alpha_.T@K_tr_te > 0).reshape(-1, ).astype(\"int\") - 1\n",
        "        \n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        inputs:\n",
        "        - X (size N_texd): the points in R^d we want to classify\n",
        "        - y (size N_tex1): the labels of the points\n",
        "        \"\"\"\n",
        "        y_pred = self.predict(X)\n",
        "        \n",
        "        return np.sum(y_pred == y)/y.shape[0]"
      ],
      "id": "AHIjYIWA0a0S",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgsnnyjd0a0S",
        "outputId": "a442b499-687a-45c1-e093-5c024d87d119"
      },
      "source": [
        "dim = 100\n",
        "Nb_samples = 2000\n",
        "prop_test = 0.2\n",
        "lamb = 0.5\n",
        "sigma = 1.2\n",
        "\n",
        "all_y_eval = []\n",
        "\n",
        "np.random.seed(1)\n",
        "for name in [0, 1, 2]:\n",
        "    # Data Processing\n",
        "    X = pd.read_csv(f'{data_folder}/Xtr{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
        "    y = pd.read_csv(f'{data_folder}/Ytr{name}.csv')\n",
        "    y = y[\"Bound\"].to_numpy()\n",
        "    y[y==0] = -1\n",
        "    \n",
        "    mean, std = X.mean(axis=0), X.std(axis=0)\n",
        "    X = (X - mean)/std\n",
        "\n",
        "    tr_indices = np.random.choice(Nb_samples, size=int((1-prop_test)*Nb_samples), replace=False)\n",
        "    te_indices = [idx for idx in range(Nb_samples) if idx not in tr_indices]\n",
        "\n",
        "    X_tr = X[tr_indices]\n",
        "    X_te = X[te_indices]\n",
        "    \n",
        "    y_tr = y[tr_indices]\n",
        "    y_te = y[te_indices]\n",
        "    \n",
        "    assert X_tr.shape[0] + X_te.shape[0] == X.shape[0]\n",
        "    assert y_tr.shape[0] + y_te.shape[0] == y.shape[0]\n",
        "    \n",
        "    # Fitting\n",
        "    params = {'lamb': np.linspace(0.1, 2, 2), 'sigma': np.linspace(0.5, 2, 20), 'kernel': ['gaussian']}\n",
        "    krr = GridSearchCV(KernelRidgeRegressor(), params)\n",
        "#     krr = KernelRidgeRegressor()\n",
        "    \n",
        "    krr.fit(X_tr,y_tr)\n",
        "    \n",
        "    print(krr.best_params_)\n",
        "    \n",
        "    print(f\"Accuracy on train set {name}: {krr.score(X_tr, y_tr):.2f}\")\n",
        "    print(f\"Accuracy on test set {name} : {krr.score(X_te, y_te):.2f}\")\n",
        "    \n",
        "    # Prediction on the new set\n",
        "    X_eval = pd.read_csv(f'{data_folder}/Xte{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
        "    X_eval = (X_eval - mean)/std\n",
        "    y_eval = krr.predict(X_eval)\n",
        "    all_y_eval.append(y_eval)\n",
        "    \n",
        "all_y_eval = np.hstack(all_y_eval).reshape(-1)"
      ],
      "id": "kgsnnyjd0a0S",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'kernel': 'gaussian', 'lamb': 0.1, 'sigma': 0.5789473684210527}\n",
            "Accuracy on train set 0: 1.00\n",
            "Accuracy on test set 0 : 0.57\n",
            "{'kernel': 'gaussian', 'lamb': 0.1, 'sigma': 0.6578947368421053}\n",
            "Accuracy on train set 1: 1.00\n",
            "Accuracy on test set 1 : 0.59\n",
            "{'kernel': 'gaussian', 'lamb': 0.1, 'sigma': 0.5}\n",
            "Accuracy on train set 2: 1.00\n",
            "Accuracy on test set 2 : 0.67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9m6sPN00a0S"
      },
      "source": [
        "### 2. Kernel SVM"
      ],
      "id": "z9m6sPN00a0S"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhSUDIUp0a0S"
      },
      "source": [
        "class KernelSVM(BaseEstimator, ClassifierMixin):\n",
        "    \n",
        "    def __init__(self, lamb=1., sigma=1., k = 3, kernel='gaussian'):\n",
        "        \"\"\"\n",
        "        This class implements methods for fitting and predicting with a KernelRidgeRegressor used for classification \n",
        "        (by thresholding the value regressed). Any kernel can be used. \n",
        "        inputs:\n",
        "        - lamb : the regularisation parameter \n",
        "        - sigma : the parameter of the Gaussian kernel (if Gaussian kernel selected)\n",
        "        - kernel : the kernel we consider\n",
        "        \"\"\"\n",
        "        self.lamb = lamb\n",
        "        self.sigma = sigma\n",
        "        self.k = k\n",
        "        self.kernel = kernel\n",
        "        if self.kernel == 'gaussian':\n",
        "            self.kernel_ = partial(Gaussian_kernel, sig=sigma)\n",
        "        elif self.kernel == 'spectrum':\n",
        "            self.kernel_ = partial(Spectrum_kernel, k=k)\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Kernel {self.kernel} is not implemented yet\")\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        inputs:\n",
        "        - X (size: N_trxd): the points of the training set\n",
        "        - y (size: N_trx1): the values of the classes\n",
        "        \"\"\"\n",
        "        # We keep values of training in memory for prediction\n",
        "        N_tr = X.shape[0]\n",
        "        self.X_tr_ = np.copy(X)\n",
        "\n",
        "        if self.kernel == 'gaussian':\n",
        "          K = self.kernel_(X, X, sig=self.sigma)\n",
        "        elif self.kernel == 'spectrum':\n",
        "          K = self.kernel_(X, X, k=self.k)\n",
        "        # Define QP and solve it with cvxpy\n",
        "        alpha = cp.Variable(N_tr)\n",
        "        objective = cp.Maximize(2*alpha.T@y - cp.quad_form(alpha, K))\n",
        "        constraints = [0 <= cp.multiply(y,alpha), cp.multiply(y,alpha) <= 1/(2*self.lamb*N_tr)]\n",
        "        prob = cp.Problem(objective, constraints)\n",
        "\n",
        "        # The optimal objective value is returned by `prob.solve()`.\n",
        "        result = prob.solve()\n",
        "        # The optimal value for x is stored in `x.value`.\n",
        "        self.alpha_ = alpha.value\n",
        "        \n",
        "        return self\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        inputs:\n",
        "        - X (size N_texd): the points in R^d we want to classify\n",
        "        output:\n",
        "         - the predicted class for the associated y given the\n",
        "        Linear Regression parameters\n",
        "        \"\"\"\n",
        "        if self.kernel == 'gaussian':\n",
        "          K_tr_te = self.kernel_(self.X_tr_, X, sig=self.sigma)\n",
        "        elif self.kernel == 'spectrum':\n",
        "          K_tr_te = self.kernel_(self.X_tr_, X, k=self.k)\n",
        "        \n",
        "        return 2 * (self.alpha_.T@K_tr_te > 0).reshape(-1, ).astype(\"int\") - 1\n",
        "        \n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        inputs:\n",
        "        - X (size N_texd): the points in R^d we want to classify\n",
        "        - y (size N_tex1): the labels of the points\n",
        "        \"\"\"\n",
        "        y_pred = self.predict(X)\n",
        "        \n",
        "        return np.sum(y_pred == y)/y.shape[0]"
      ],
      "id": "dhSUDIUp0a0S",
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTsajMKQSp2_"
      },
      "source": [
        "#### Gaussian Kernel SVM"
      ],
      "id": "JTsajMKQSp2_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNl7NJXm0a0S"
      },
      "source": [
        "dim = 100\n",
        "Nb_samples = 2000\n",
        "prop_test = 0.2\n",
        "lamb = 0.5\n",
        "sigma = 1.2\n",
        "\n",
        "all_y_eval = []\n",
        "\n",
        "np.random.seed(1)\n",
        "for name in [0, 1, 2]:\n",
        "    # Data Processing\n",
        "    X = pd.read_csv(f'{data_folder}/Xtr{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
        "    y = pd.read_csv(f'{data_folder}/Ytr{name}.csv')\n",
        "    y = y[\"Bound\"].to_numpy()\n",
        "    y[y==0] = -1\n",
        "    \n",
        "    mean, std = X.mean(axis=0), X.std(axis=0)\n",
        "    X = (X - mean)/std\n",
        "\n",
        "    tr_indices = np.random.choice(Nb_samples, size=int((1-prop_test)*Nb_samples), replace=False)\n",
        "    te_indices = [idx for idx in range(Nb_samples) if idx not in tr_indices]\n",
        "\n",
        "    X_tr = X[tr_indices]\n",
        "    X_te = X[te_indices]\n",
        "    \n",
        "    y_tr = y[tr_indices]\n",
        "    y_te = y[te_indices]\n",
        "    \n",
        "    assert X_tr.shape[0] + X_te.shape[0] == X.shape[0]\n",
        "    assert y_tr.shape[0] + y_te.shape[0] == y.shape[0]\n",
        "    \n",
        "    # Fitting\n",
        "    params = {'lamb': np.logspace(-10., -7., 4), 'sigma': np.logspace(-1., 2., 4), 'kernel': ['gaussian']}\n",
        "    ksvm = GridSearchCV(KernelSVM(), params)\n",
        "#     krr = KernelRidgeRegressor()\n",
        "    \n",
        "    ksvm.fit(X_tr,y_tr)\n",
        "    \n",
        "    print(ksvm.best_params_)\n",
        "    \n",
        "    print(f\"Accuracy on train set {name}: {ksvm.score(X_tr, y_tr):.2f}\")\n",
        "    print(f\"Accuracy on test set {name} : {ksvm.score(X_te, y_te):.2f}\")\n",
        "    \n",
        "    # Prediction on the new set\n",
        "    X_eval = pd.read_csv(f'{data_folder}/Xte{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
        "    X_eval = (X_eval - mean)/std\n",
        "    y_eval = ksvm.predict(X_eval)\n",
        "    all_y_eval.append(y_eval)\n",
        "    \n",
        "all_y_eval = np.hstack(all_y_eval).reshape(-1)"
      ],
      "id": "YNl7NJXm0a0S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gPd2jimSvGR"
      },
      "source": [
        "#### Spectrum Kernel SVM"
      ],
      "id": "6gPd2jimSvGR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "hjpfylDEImWk",
        "outputId": "15104b61-5bd3-443c-e2cf-45382846a0ab"
      },
      "source": [
        "## Kernel SVM with Spectrum kernel\n",
        "\n",
        "dim = 100\n",
        "Nb_samples = 2000\n",
        "prop_test = 0.2\n",
        "lamb = 0.5\n",
        "sigma = 1.2\n",
        "k = 3\n",
        "\n",
        "all_y_eval = []\n",
        "\n",
        "np.random.seed(1)\n",
        "for name in [0, 1, 2]:\n",
        "    # Data Processing\n",
        "    df = pd.read_csv(f'{data_folder}/Xtr{name}.csv')\n",
        "    X = np.array(df['seq'])\n",
        "    y = pd.read_csv(f'{data_folder}/Ytr{name}.csv')\n",
        "    y = y[\"Bound\"].to_numpy()\n",
        "    y[y==0] = -1\n",
        "    \n",
        "\n",
        "    tr_indices = np.random.choice(Nb_samples, size=int((1-prop_test)*Nb_samples), replace=False)\n",
        "    te_indices = [idx for idx in range(Nb_samples) if idx not in tr_indices]\n",
        "\n",
        "    X_tr = X[tr_indices]\n",
        "    X_te = X[te_indices]\n",
        "    \n",
        "    y_tr = y[tr_indices]\n",
        "    y_te = y[te_indices]\n",
        "    \n",
        "    assert X_tr.shape[0] + X_te.shape[0] == X.shape[0]\n",
        "    assert y_tr.shape[0] + y_te.shape[0] == y.shape[0]\n",
        "    \n",
        "    # Fitting\n",
        "    # params = {'lamb': np.logspace(-10., -7., 4), 'k': np.array([3,4,5,6]), 'kernel': ['spectrum']}\n",
        "    # ksvm = GridSearchCV(KernelSVM(), params)\n",
        "#     krr = KernelRidgeRegressor()\n",
        "    ksvm = KernelSVM(lamb = lamb, k=k, kernel='spectrum')\n",
        "    \n",
        "    ksvm.fit(X_tr,y_tr)\n",
        "    \n",
        "    print(ksvm.best_params_)\n",
        "    \n",
        "    print(f\"Accuracy on train set {name}: {ksvm.score(X_tr, y_tr):.2f}\")\n",
        "    print(f\"Accuracy on test set {name} : {ksvm.score(X_te, y_te):.2f}\")\n",
        "    \n",
        "    # Prediction on the new set\n",
        "    X_eval = np.array(pd.read_csv(f'{data_folder}/Xte{name}.csv')['seq'])\n",
        "    y_eval = ksvm.predict(X_eval)\n",
        "    all_y_eval.append(y_eval)\n",
        "    \n",
        "all_y_eval = np.hstack(all_y_eval).reshape(-1)"
      ],
      "id": "hjpfylDEImWk",
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DCPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDCPError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-120-d1582ef685ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mksvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKernelSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlamb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'spectrum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mksvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mksvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-117-0dd8df7deb85>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# The optimal objective value is returned by `prob.solve()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m# The optimal value for x is stored in `x.value`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cvxpy/problems/problem.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0msolve_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msolve_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cvxpy/problems/problem.py\u001b[0m in \u001b[0;36m_solve\u001b[0;34m(self, solver, warm_start, verbose, parallel, gp, qcp, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m                     solver, warm_start, verbose, **kwargs)\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_chains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m         data, solving_inverse_data = self._solving_chain.apply(\n\u001b[1;32m    570\u001b[0m             self._intermediate_problem)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cvxpy/problems/problem.py\u001b[0m in \u001b[0;36m_construct_chains\u001b[0;34m(self, solver, gp)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     def _solve(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cvxpy/problems/problem.py\u001b[0m in \u001b[0;36m_construct_chains\u001b[0;34m(self, solver, gp)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intermediate_chain\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m                     \u001b[0mconstruct_intermediate_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_solvers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intermediate_problem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intermediate_inverse_data\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intermediate_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cvxpy/reductions/solvers/intermediate_chain.py\u001b[0m in \u001b[0;36mconstruct_intermediate_chain\u001b[0;34m(problem, candidates, gp)\u001b[0m\n\u001b[1;32m     94\u001b[0m             append += (\"\\nHowever, the problem does follow DQCP rules. \"\n\u001b[1;32m     95\u001b[0m                        \"Consider calling solve() with `qcp=True`.\")\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mDCPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Problem does not follow DCP rules. Specifically:\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mgp\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dgp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDCPError\u001b[0m: Problem does not follow DCP rules. Specifically:\nThe objective is not DCP. Its following subexpressions are not:\nQuadForm(var14131, [[329. 166. 155. ... 149. 182. 115.]\n [166. 299. 138. ... 181. 198. 261.]\n [155. 138. 349. ... 146. 148. 173.]\n ...\n [149. 181. 146. ... 279. 162. 180.]\n [182. 198. 148. ... 162. 301. 174.]\n [115. 261. 173. ... 180. 174. 425.]])"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8mljyosSk1h",
        "outputId": "61f25f56-d176-4aeb-823f-769b2ef71c9a"
      },
      "source": [
        "from itertools import product\n",
        "\n",
        "Xtr_0 = pd.read_csv('data/Xtr0.csv')\n",
        "X_train_0 = np.array(Xtr_0['seq'])\n",
        "\n",
        "SPK = Spectrum_kernel(X_train_0,X_train_0,k=3)\n",
        "np.real(np.linalg.eig(SPK)[0])"
      ],
      "id": "Y8mljyosSk1h",
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3.67373760e+05,  4.32026053e+04,  3.23499785e+04, ...,\n",
              "        7.12260801e-14, -1.66362557e-14,  2.67708609e-14])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aARgH1w10a0T"
      },
      "source": [
        "ids = np.arange(all_y_eval.shape[0])\n",
        "filename = \"results/submission_gaussian_svm.csv\"\n",
        "\n",
        "# write_csv(ids, all_y_eval, filename)"
      ],
      "id": "aARgH1w10a0T",
      "execution_count": null,
      "outputs": []
    }
  ]
}