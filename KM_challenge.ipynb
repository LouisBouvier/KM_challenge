{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Methods: Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia Linhart, Roman Castagné, Louis Bouvier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(ids, labels, filename):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        - ids: list of ids, should be an increasing list of integers\n",
    "        - labels: list of corresponding labels, either 0 or 1\n",
    "        - file: string containing the name that should be given to the submission file    \n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"Id\": ids, \"Bound\": labels})\n",
    "    df.to_csv(filename, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I) Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data' # 'machine-learning-with-kernel-methods-2021'\n",
    "\n",
    "X_train_1 = pd.read_csv(f'{data_folder}/Xtr2_mat100.csv', sep = ' ', index_col=False, header=None)\n",
    "y_train_1 = pd.read_csv(f'{data_folder}/Ytr2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_1 = np.array(y_train_1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = np.array(X_train_1)\n",
    "print(X_train_1.shape)\n",
    "X_train_1 = (X_train_1 - X_train_1.mean(axis=0))/X_train_1.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) First linear models of the mat100 input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(z):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    - z (any size): an array-like element\n",
    "    ouput:\n",
    "    - the element-wize application of the sigmoïd function on z\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X,y,w,b):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: dxN): the points we want to classify\n",
    "    - y (size: 1xN): the values of the classes\n",
    "    - w (size: dx1): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    output:\n",
    "    - the opposite of the log-likelihood of the Logistic Regression model computed with respect to\n",
    "    the points (X,y) and the parameters w,b\n",
    "    \"\"\"\n",
    "    X_tilde = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "    w_tilde = np.hstack((w,b))\n",
    "    return -np.sum(y * np.log(g(w_tilde@X_tilde.T)) + (1-y) * np.log(1-g(w_tilde@X_tilde.T)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad(X,y,w,b):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: Nxd): the points we want to classify\n",
    "    - y (size: Nx1): the values of the classes\n",
    "    - w (size: 1xd): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    output:\n",
    "    - the gradient of the loss of the Logistic Regression model computed \n",
    "    with respect to (w,b) = w_tilde having the points (X,y) \n",
    "    \"\"\"\n",
    "    X_tilde = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "    w_tilde = np.hstack((w,b))\n",
    "    return -X_tilde.T @ (y - g(w_tilde@X_tilde.T).reshape(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hess(X,y,w,b):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: Nxd): the points we want to classify\n",
    "    - y (size: Nx1): the values of the classes\n",
    "    - w (size: d): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    output:\n",
    "    - the hessian of the loss of the Logistic Regression model computed \n",
    "    with respect to (w,b) = w_tilde having the points (X,y) \n",
    "    \"\"\"\n",
    "    X_tilde = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "    w_tilde = np.hstack((w,b))    \n",
    "    temp = (g(w_tilde @ X_tilde.T) * (g(w_tilde @ X_tilde.T) - 1)).reshape(-1,)\n",
    "    return -X_tilde.T @ np.diag(temp) @ X_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(X,y,w,b,delta,grad,alpha=0.1,beta=0.7):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: Nxd): the points we want to classify\n",
    "    - y (size: Nxd): the values of the classes\n",
    "    - w (size: d): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    - delta (size n): direction of the search\n",
    "    - grad (size n): value of the gradient at point (w,b)\n",
    "    - alpha: factor of the slope of the line in the backtracking line search\n",
    "    - beta: factor of reduction of the step length\n",
    "    \n",
    "    outputs:\n",
    "    - t: the step length for the Newton step on the objective function\n",
    "    computed with backtracking line search towards delta\"\"\"\n",
    "        \n",
    "    t = 1\n",
    "    while(compute_loss(X, y, w+t*delta[:-1], b+t*delta[-1])>\n",
    "            compute_loss(X,y,w,b) + alpha*t*grad.T @ delta):\n",
    "        t = beta*t\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton(X, y, w0, b0, eps=pow(10,-1)):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: Nxd): the points we want to classify\n",
    "    - y (size: Nx1): the values of the classes\n",
    "    - w0 (size: d): the initial weights of the affine mapping of x\n",
    "    - b0 (size: 1x1): the initial constant of the affine mapping of x\n",
    "    output:\n",
    "    - the paramer vector w_tilde_hat = (w_hat, b_hat) which maximizes the log-likelihood of \n",
    "    the sample (X,y) in the Logistic Regression model (or minimizes the loss)\n",
    "    - the cached values of the loss evaluated along training\n",
    "    \"\"\"\n",
    "    w_, b_ = w0, b0\n",
    "    grad = compute_grad(X, y, w0, b0)\n",
    "    hess = compute_hess(X, y, w0, b0)\n",
    "    \n",
    "#     inv_hess = np.linalg.inv(compute_hess(X,y,w0,b0))\n",
    "    inv_hess, _, _, _ = np.linalg.lstsq(hess, np.eye(hess.shape[0]))\n",
    "    dec_2 = grad.T@inv_hess@grad\n",
    "    Loss_hist = [compute_loss(X,y,w0,b0)]\n",
    "    while dec_2/2 > eps: # condition on the Newton decrement\n",
    "        grad = compute_grad(X,y,w_,b_)\n",
    "        hess = compute_hess(X,y,w_,b_)\n",
    "        \n",
    "#         inv_hess = np.linalg.inv(compute_hess(X,y,w_,b_))\n",
    "        inv_hess, _, _, _ = np.linalg.lstsq(hess, np.eye(hess.shape[0]))\n",
    "        dec_2 = grad.T@inv_hess@grad\n",
    "        delta = - inv_hess@grad\n",
    "        t_bt = backtracking(X, y, w_, b_, delta, grad)\n",
    "        w_ = w_ + t_bt*delta[:-1]\n",
    "        b_ = b_ + t_bt*delta[-1]\n",
    "        Loss_hist.append(compute_loss(X,y,w_,b_))\n",
    "    return w_, b_, Loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_LogReg(x,w,b):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - x (size dx1): a point in R^d\n",
    "    - w (size: dx1): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    output:\n",
    "     - the predicted class for the associated y given the\n",
    "    Logistic Regression parameters\n",
    "    \"\"\"    \n",
    "    return (w.T @ x+b > 0).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressor(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, lamb=1.):\n",
    "        \"\"\"\n",
    "        This class implements methods for fitting and predicting with a RidgeRegressor used for classification \n",
    "        (by thresholding the value regressed).\n",
    "        inputs:\n",
    "        - lamb : the regularisation parameter\n",
    "        \"\"\"\n",
    "        self.lamb = lamb\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - X (size: Nxd): the points we want to classify\n",
    "        - y (size: Nx1): the values of the classes\n",
    "        outputs:\n",
    "        - the value of MLE estimation (w_hat, b_hat) in the Linear regression model\n",
    "        \"\"\"\n",
    "        w0, b0 = np.random.randn(1, 100)*0.07, np.zeros((1,1))\n",
    "        self.w_, self.b_, _ = Newton(X, y, w0, b0)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - x (size Nxd): a point in R^d\n",
    "        - w (size: 1xd): the weights of the affine mapping of x\n",
    "        - b (size: 1x1): the constant of the affine mapping of x\n",
    "        output:\n",
    "         - the predicted class for the associated y given the\n",
    "        Linear Regression parameters\n",
    "        \"\"\"    \n",
    "        return (self.w_@X.T + self.b_ > 1/2).astype(\"int\")\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - X (size Nxd): the points in R^d we want to classify\n",
    "        - y (size Nx1): the labels of the points\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.sum(y_pred == y)/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dim = 100\n",
    "Nb_samples = 2000\n",
    "prop_test = 0.05\n",
    "\n",
    "all_y_eval = []\n",
    "\n",
    "np.random.seed(1)\n",
    "for name in [0, 1, 2]:\n",
    "    X = pd.read_csv(f'{data_folder}/Xtr{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
    "    y = pd.read_csv(f'{data_folder}/Ytr{name}.csv')\n",
    "    y = y[\"Bound\"].to_numpy()\n",
    "    \n",
    "    mean, std = X.mean(axis=0), X.std(axis=0)\n",
    "    X = (X - mean)/std\n",
    "\n",
    "    tr_indices = np.random.choice(Nb_samples, size=int((1-prop_test)*Nb_samples), replace=False)\n",
    "    te_indices = [idx for idx in range(Nb_samples) if idx not in tr_indices]\n",
    "\n",
    "    X_tr = X[tr_indices]\n",
    "    X_te = X[te_indices]\n",
    "    \n",
    "    y_tr = y[tr_indices]\n",
    "    y_te = y[te_indices]\n",
    "    \n",
    "    assert X_tr.shape[0] + X_te.shape[0] == X.shape[0]\n",
    "    assert y_tr.shape[0] + y_te.shape[0] == y.shape[0]\n",
    "    \n",
    "    # Fitting\n",
    "    logreg = LogisticRegressor()\n",
    "    \n",
    "    logreg.fit(X_tr, y_tr)\n",
    "    \n",
    "    print(f\"Accuracy on train set {name}: {logreg.score(X_tr, y_tr):.2f}\")\n",
    "    print(f\"Accuracy on test set {name} : {logreg.score(X_te, y_te):.2f}\")   \n",
    "    \n",
    "    # Prediction on the new set\n",
    "    X_eval = pd.read_csv(f'{data_folder}/Xte{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
    "    X_eval = (X_eval - mean)/std\n",
    "    y_eval = rr.predict(X_eval)\n",
    "    all_y_eval.append(y_eval)\n",
    "    \n",
    "all_y_eval = np.hstack(all_y_eval).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ids = np.arange(all_y_eval.shape[0])\n",
    "filename = \"results/submission_log_reg.csv\"\n",
    "\n",
    "# write_csv(ids, all_y_eval, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_RR_MLE(X,y,lamb):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: dxN): the points we want to classify\n",
    "    - y (size: 1xN): the values of the classes\n",
    "    outputs:\n",
    "    - the value of MLE estimation (w_hat, b_hat) in the Linear regression model\n",
    "    \"\"\"\n",
    "    X_tilde = np.vstack((X,np.ones(X.shape[1])))\n",
    "    temp = np.linalg.inv(X_tilde@X_tilde.T + lamb*X.shape[1]*np.eye(1+X.shape[0]))@X_tilde@y.T\n",
    "    return temp[:-1], temp[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_RR(x,w,b):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - x (size dx1): a point in R^d\n",
    "    - w (size: dx1): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    output:\n",
    "     - the predicted class for the associated y given the\n",
    "    Linear Regression parameters\n",
    "    \"\"\"    \n",
    "    return (w.T@x+b>1/2).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegressor(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, lamb=1.):\n",
    "        \"\"\"\n",
    "        This class implements methods for fitting and predicting with a RidgeRegressor used for classification \n",
    "        (by thresholding the value regressed).\n",
    "        inputs:\n",
    "        - lamb : the regularisation parameter\n",
    "        \"\"\"\n",
    "        self.lamb = lamb\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - X (size: Nxd): the points we want to classify\n",
    "        - y (size: Nx1): the values of the classes\n",
    "        outputs:\n",
    "        - the value of MLE estimation (w_hat, b_hat) in the Linear regression model\n",
    "        \"\"\"\n",
    "        X_tilde = np.hstack((X, np.ones((X.shape[0], 1))))\n",
    "        temp = np.linalg.inv(X_tilde.T @ X_tilde + self.lamb * X.shape[0] * np.eye(X_tilde.shape[1])) @ (X_tilde.T @ y)\n",
    "        self.w_ = temp[:-1]\n",
    "        self.b_ = temp[-1]\n",
    "\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - x (size Nxd): a point in R^d\n",
    "        - w (size: 1xd): the weights of the affine mapping of x\n",
    "        - b (size: 1x1): the constant of the affine mapping of x\n",
    "        output:\n",
    "         - the predicted class for the associated y given the\n",
    "        Linear Regression parameters\n",
    "        \"\"\"    \n",
    "        return (self.w_@X.T + self.b_ > 1/2).astype(\"int\")\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - X (size Nxd): the points in R^d we want to classify\n",
    "        - y (size Nx1): the labels of the points\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.sum(y_pred == y)/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 100\n",
    "Nb_samples = 2000\n",
    "prop_test = 0.05\n",
    "lamb = 0.1\n",
    "\n",
    "all_y_eval = []\n",
    "\n",
    "np.random.seed(1)\n",
    "for name in [0, 1, 2]:\n",
    "    # Data processing\n",
    "    X = pd.read_csv(f'{data_folder}/Xtr{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
    "    y = pd.read_csv(f'{data_folder}/Ytr{name}.csv')\n",
    "    y = y[\"Bound\"].to_numpy()\n",
    "    \n",
    "    mean, std = X.mean(axis=0), X.std(axis=0)\n",
    "    X = (X - mean)/std\n",
    "\n",
    "    tr_indices = np.random.choice(Nb_samples, size=int((1-prop_test)*Nb_samples), replace=False)\n",
    "    te_indices = [idx for idx in range(Nb_samples) if idx not in tr_indices]\n",
    "\n",
    "    X_tr = X[tr_indices]\n",
    "    X_te = X[te_indices]\n",
    "    \n",
    "    y_tr = y[tr_indices]\n",
    "    y_te = y[te_indices]\n",
    "    \n",
    "    assert X_tr.shape[0] + X_te.shape[0] == X.shape[0]\n",
    "    assert y_tr.shape[0] + y_te.shape[0] == y.shape[0]\n",
    "    \n",
    "    # Fitting the classifier\n",
    "    params = {'lamb': np.linspace(0.001, 0.1, 20)}\n",
    "    rr = GridSearchCV(RidgeRegressor(), params)\n",
    "#     rr = RidgeRegressor(lamb=lamb)\n",
    "\n",
    "    rr.fit(X_tr, y_tr)\n",
    "    \n",
    "    print(rr.best_params_)\n",
    "    \n",
    "    print(f\"Accuracy on train set {name}: {rr.score(X_tr, y_tr):.2f}\")\n",
    "    print(f\"Accuracy on test set {name} : {rr.score(X_te, y_te):.2f}\")\n",
    "    \n",
    "    # Prediction on the new set\n",
    "    X_eval = pd.read_csv(f'{data_folder}/Xte{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
    "    X_eval = (X_eval - mean)/std\n",
    "    y_eval = rr.predict(X_eval)\n",
    "    all_y_eval.append(y_eval)\n",
    "    \n",
    "all_y_eval = np.hstack(all_y_eval).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III) Kernel baselines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian_kernel(X1, X2, sig):\n",
    "    \"\"\"inputs:\n",
    "    - X1 (size dxN1): a set of points\n",
    "    - X2 (size dxN2): another one  \n",
    "    - sig (float): the std of the kernel\n",
    "    ouput:\n",
    "    - the associated (N1xN2) Gaussian kernel\n",
    "    \"\"\"\n",
    "    return np.exp(-distance_matrix(X1,X2)/(2*sig**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_KRR_MLE(X, y, lamb, sig=10):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: dxN): the points we want to classify\n",
    "    - y (size: 1xN): the values of the classes\n",
    "    outputs:\n",
    "    - the value of MLE estimation (w_hat, b_hat) in the Linear regression model\n",
    "    \"\"\"\n",
    "    K = Gaussian_kernel(X, X, sig=sig)\n",
    "    alpha = np.linalg.inv(K+lamb*X.shape[1]*np.eye(X.shape[1]))@y.T\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_KRR(X_tr, X_te, alpha, sig=10):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X_tr (size dx1): a point in R^d\n",
    "    - w (size: dx1): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    output:\n",
    "     - the predicted class for the associated y given the\n",
    "    Linear Regression parameters\n",
    "    \"\"\"    \n",
    "    K_te_tr = Gaussian_kernel(X_tr, X_te, sig=sig)\n",
    "    return 2*(alpha.T@K_te_tr>0).astype(\"int\")-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelRidgeRegressor(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, lamb=1., sigma=1., kernel='gaussian'):\n",
    "        \"\"\"\n",
    "        This class implements methods for fitting and predicting with a KernelRidgeRegressor used for classification \n",
    "        (by thresholding the value regressed). Any kernel can be used. \n",
    "        inputs:\n",
    "        - lamb : the regularisation parameter \n",
    "        - sigma :\n",
    "        - kernel : \n",
    "        \"\"\"\n",
    "        self.lamb = lamb\n",
    "        self.sigma = sigma\n",
    "        self.kernel = kernel\n",
    "        if self.kernel == 'gaussian':\n",
    "            self.kernel_ = partial(Gaussian_kernel, sig=sigma)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Kernel {self.kernel} is not implemented yet\")\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - X (size: dxN): the points we want to classify\n",
    "        - y (size: 1xN): the values of the classes\n",
    "        \"\"\"\n",
    "        # We keep values of training in memory for prediction\n",
    "        self.X_tr_ = np.copy(X)\n",
    "        K = self.kernel_(X, X, sig=self.sigma)\n",
    "        self.alpha_ = np.linalg.inv(K+self.lamb*X.shape[0]*np.eye(X.shape[0]))@y\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - X (size dxnum_samples): the points in R^d we want to classify\n",
    "        output:\n",
    "         - the predicted class for the associated y given the\n",
    "        Linear Regression parameters\n",
    "        \"\"\"\n",
    "        K_tr_te = self.kernel_(self.X_tr_, X, sig=self.sigma)\n",
    "        \n",
    "        return 2 * (self.alpha_.T@K_tr_te > 0).reshape(-1, 1).astype(\"int\") - 1\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - X (size Nxd): the points in R^d we want to classify\n",
    "        - y (size Nx1): the labels of the points\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        return np.sum(y_pred == y)/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dim = 100\n",
    "Nb_samples = 2000\n",
    "prop_test = 0.2\n",
    "lamb = 0.5\n",
    "sigma = 1.2\n",
    "\n",
    "all_y_eval = []\n",
    "\n",
    "np.random.seed(1)\n",
    "for name in [0, 1, 2]:\n",
    "    # Data Processing\n",
    "    X = pd.read_csv(f'{data_folder}/Xtr{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
    "    y = pd.read_csv(f'{data_folder}/Ytr{name}.csv')\n",
    "    y = y[\"Bound\"].to_numpy()\n",
    "    y[y==0] = -1\n",
    "    \n",
    "    mean, std = X.mean(axis=0), X.std(axis=0)\n",
    "    X = (X - mean)/std\n",
    "\n",
    "    tr_indices = np.random.choice(Nb_samples, size=int((1-prop_test)*Nb_samples), replace=False)\n",
    "    te_indices = [idx for idx in range(Nb_samples) if idx not in tr_indices]\n",
    "\n",
    "    X_tr = X[tr_indices]\n",
    "    X_te = X[te_indices]\n",
    "    \n",
    "    y_tr = y[tr_indices]\n",
    "    y_te = y[te_indices]\n",
    "    \n",
    "    assert X_tr.shape[0] + X_te.shape[0] == X.shape[0]\n",
    "    assert y_tr.shape[0] + y_te.shape[0] == y.shape[0]\n",
    "    \n",
    "    # Fitting\n",
    "    params = {'lamb': np.linspace(0.1, 2, 2), 'sigma': np.linspace(0.5, 2, 2), 'kernel': ['gaussian']}\n",
    "    krr = GridSearchCV(KernelRidgeRegressor(), params)\n",
    "#     krr = KernelRidgeRegressor()\n",
    "    \n",
    "    krr.fit(X_tr,y_tr)\n",
    "    \n",
    "    print(krr.best_params_)\n",
    "    \n",
    "    print(f\"Accuracy on train set {name}: {krr.score(X_tr, y_tr):.2f}\")\n",
    "    print(f\"Accuracy on test set {name} : {krr.score(X_te, y_te):.2f}\")\n",
    "    \n",
    "    # Prediction on the new set\n",
    "    X_eval = pd.read_csv(f'{data_folder}/Xte{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
    "    X_eval = (X_eval - mean)/std\n",
    "    y_eval = krr.predict(X_eval)\n",
    "    all_y_eval.append(y_eval)\n",
    "    \n",
    "all_y_eval = np.hstack(all_y_eval).reshape(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
