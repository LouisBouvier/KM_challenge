{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Pz797qSo1Wd9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/MVA/KKML')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqUQTmZ20az9"
   },
   "source": [
    "# Kernel Methods: Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAISFXMk0a0D"
   },
   "source": [
    "Julia Linhart, Roman Castagné, Louis Bouvier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXbC5eRC0a0D"
   },
   "source": [
    "Preliminary functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZvAm-YyB0a0E"
   },
   "outputs": [],
   "source": [
    "def write_csv(ids, labels, filename):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        - ids: list of ids, should be an increasing list of integers\n",
    "        - labels: list of corresponding labels, either 0 or 1\n",
    "        - file: string containing the name that should be given to the submission file    \n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"Id\": ids, \"Bound\": labels})\n",
    "    df[\"Bound\"] = df[\"Bound\"].replace([-1], 0)\n",
    "    df.to_csv(filename, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiFKdzlB0a0E"
   },
   "source": [
    "# I) Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZKNsQc940a0F"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import cvxpy as cp\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XrXWzksi0a0F"
   },
   "outputs": [],
   "source": [
    "data_folder = 'data' # 'machine-learning-with-kernel-methods-2021'\n",
    "\n",
    "X_train_1 = pd.read_csv(f'{data_folder}/Xtr2_mat100.csv', sep = ' ', index_col=False, header=None)\n",
    "y_train_1 = pd.read_csv(f'{data_folder}/Ytr2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "8SY4m9HA0a0F",
    "outputId": "bb2b1fd8-36f1-4ec4-97ee-40dc6aeac607"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Bound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4999.500000</td>\n",
       "      <td>0.498500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>577.494589</td>\n",
       "      <td>0.500123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4499.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4999.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5499.250000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5999.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id        Bound\n",
       "count  2000.000000  2000.000000\n",
       "mean   4999.500000     0.498500\n",
       "std     577.494589     0.500123\n",
       "min    4000.000000     0.000000\n",
       "25%    4499.750000     0.000000\n",
       "50%    4999.500000     0.000000\n",
       "75%    5499.250000     1.000000\n",
       "max    5999.000000     1.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JcoPE7dE0a0H"
   },
   "outputs": [],
   "source": [
    "y_train_1 = np.array(y_train_1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "bgo-XP0u0a0I",
    "outputId": "4fa3e853-5782-4b65-a4a0-eb8906fd7df5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.010565</td>\n",
       "      <td>0.010201</td>\n",
       "      <td>0.010375</td>\n",
       "      <td>0.011587</td>\n",
       "      <td>0.011609</td>\n",
       "      <td>0.010707</td>\n",
       "      <td>0.009359</td>\n",
       "      <td>0.011957</td>\n",
       "      <td>0.009571</td>\n",
       "      <td>0.010582</td>\n",
       "      <td>0.009424</td>\n",
       "      <td>0.009793</td>\n",
       "      <td>0.012848</td>\n",
       "      <td>0.012092</td>\n",
       "      <td>0.011196</td>\n",
       "      <td>0.010364</td>\n",
       "      <td>0.009875</td>\n",
       "      <td>0.010962</td>\n",
       "      <td>0.010185</td>\n",
       "      <td>0.008342</td>\n",
       "      <td>0.010734</td>\n",
       "      <td>0.010038</td>\n",
       "      <td>0.011554</td>\n",
       "      <td>0.008995</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>0.008647</td>\n",
       "      <td>0.008886</td>\n",
       "      <td>0.008826</td>\n",
       "      <td>0.007821</td>\n",
       "      <td>0.009761</td>\n",
       "      <td>0.008533</td>\n",
       "      <td>0.011864</td>\n",
       "      <td>0.009299</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>0.009560</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.010217</td>\n",
       "      <td>0.009641</td>\n",
       "      <td>0.009880</td>\n",
       "      <td>0.010038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.010614</td>\n",
       "      <td>0.011957</td>\n",
       "      <td>0.009641</td>\n",
       "      <td>0.011772</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.008783</td>\n",
       "      <td>0.010005</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.009147</td>\n",
       "      <td>0.013565</td>\n",
       "      <td>0.010587</td>\n",
       "      <td>0.009793</td>\n",
       "      <td>0.010908</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.009772</td>\n",
       "      <td>0.009103</td>\n",
       "      <td>0.010147</td>\n",
       "      <td>0.008587</td>\n",
       "      <td>0.010538</td>\n",
       "      <td>0.010897</td>\n",
       "      <td>0.008913</td>\n",
       "      <td>0.008630</td>\n",
       "      <td>0.008380</td>\n",
       "      <td>0.009016</td>\n",
       "      <td>0.011478</td>\n",
       "      <td>0.008832</td>\n",
       "      <td>0.009989</td>\n",
       "      <td>0.010587</td>\n",
       "      <td>0.008625</td>\n",
       "      <td>0.007951</td>\n",
       "      <td>0.009457</td>\n",
       "      <td>0.008554</td>\n",
       "      <td>0.009283</td>\n",
       "      <td>0.008261</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.011141</td>\n",
       "      <td>0.009777</td>\n",
       "      <td>0.008217</td>\n",
       "      <td>0.008565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.012278</td>\n",
       "      <td>0.010723</td>\n",
       "      <td>0.011467</td>\n",
       "      <td>0.011453</td>\n",
       "      <td>0.012182</td>\n",
       "      <td>0.010478</td>\n",
       "      <td>0.009789</td>\n",
       "      <td>0.012444</td>\n",
       "      <td>0.013805</td>\n",
       "      <td>0.013652</td>\n",
       "      <td>0.012934</td>\n",
       "      <td>0.011163</td>\n",
       "      <td>0.027178</td>\n",
       "      <td>0.018160</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.010356</td>\n",
       "      <td>0.010089</td>\n",
       "      <td>0.019951</td>\n",
       "      <td>0.010631</td>\n",
       "      <td>0.009920</td>\n",
       "      <td>0.011238</td>\n",
       "      <td>0.010962</td>\n",
       "      <td>0.011475</td>\n",
       "      <td>0.009723</td>\n",
       "      <td>0.010922</td>\n",
       "      <td>0.009933</td>\n",
       "      <td>0.009622</td>\n",
       "      <td>0.009861</td>\n",
       "      <td>0.010099</td>\n",
       "      <td>0.010628</td>\n",
       "      <td>0.009945</td>\n",
       "      <td>0.010829</td>\n",
       "      <td>0.010358</td>\n",
       "      <td>0.010460</td>\n",
       "      <td>0.011039</td>\n",
       "      <td>0.009612</td>\n",
       "      <td>0.010705</td>\n",
       "      <td>0.012258</td>\n",
       "      <td>0.020208</td>\n",
       "      <td>0.011266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010436</td>\n",
       "      <td>0.011172</td>\n",
       "      <td>0.012915</td>\n",
       "      <td>0.010912</td>\n",
       "      <td>0.011305</td>\n",
       "      <td>0.016977</td>\n",
       "      <td>0.014644</td>\n",
       "      <td>0.012108</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.009647</td>\n",
       "      <td>0.011868</td>\n",
       "      <td>0.011752</td>\n",
       "      <td>0.013102</td>\n",
       "      <td>0.010237</td>\n",
       "      <td>0.009652</td>\n",
       "      <td>0.009687</td>\n",
       "      <td>0.011871</td>\n",
       "      <td>0.010457</td>\n",
       "      <td>0.012348</td>\n",
       "      <td>0.011010</td>\n",
       "      <td>0.011005</td>\n",
       "      <td>0.010695</td>\n",
       "      <td>0.009248</td>\n",
       "      <td>0.010494</td>\n",
       "      <td>0.009279</td>\n",
       "      <td>0.011204</td>\n",
       "      <td>0.010571</td>\n",
       "      <td>0.015973</td>\n",
       "      <td>0.009745</td>\n",
       "      <td>0.011904</td>\n",
       "      <td>0.009605</td>\n",
       "      <td>0.009701</td>\n",
       "      <td>0.009350</td>\n",
       "      <td>0.009741</td>\n",
       "      <td>0.012341</td>\n",
       "      <td>0.010338</td>\n",
       "      <td>0.010863</td>\n",
       "      <td>0.010402</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>0.009283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.076087</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.184783</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.141304</td>\n",
       "      <td>0.076087</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.141304</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.076087</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.076087</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.119565</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.141304</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.119565</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.119565</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.076087</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.076087</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.076087</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.076087</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1   ...           98           99\n",
       "count  2000.000000  2000.000000  ...  2000.000000  2000.000000\n",
       "mean      0.010565     0.010201  ...     0.008217     0.008565\n",
       "std       0.012278     0.010723  ...     0.009709     0.009283\n",
       "min       0.000000     0.000000  ...     0.000000     0.000000\n",
       "25%       0.000000     0.000000  ...     0.000000     0.000000\n",
       "50%       0.010870     0.010870  ...     0.010870     0.010870\n",
       "75%       0.010870     0.021739  ...     0.010870     0.010870\n",
       "max       0.086957     0.065217  ...     0.065217     0.043478\n",
       "\n",
       "[8 rows x 100 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZJU9wcKY0a0I",
    "outputId": "0f85cc84-d6c1-4259-a9fd-8eae7acdf52e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train_1 = np.array(X_train_1)\n",
    "print(X_train_1.shape)\n",
    "X_train_1 = (X_train_1 - X_train_1.mean(axis=0))/X_train_1.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8G8Z3xuG0a0J",
    "outputId": "cb0bb08c-a8d5-4eda-f723-847c647f955b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJFmTps10a0J"
   },
   "source": [
    "# II) First linear models of the mat100 input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovYDI_gw0a0J"
   },
   "source": [
    "## A) Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_E7S-54J0a0K"
   },
   "outputs": [],
   "source": [
    "def g(z):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    - z (any size): an array-like element\n",
    "    ouput:\n",
    "    - the element-wize application of the sigmoïd function on z\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ktw_MldY0a0K"
   },
   "outputs": [],
   "source": [
    "def compute_loss(X,y,w,b):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: Nxd): the points we want to classify\n",
    "    - y (size: Nx1): the values of the classes\n",
    "    - w (size: 1xd): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    output:\n",
    "    - the opposite of the log-likelihood of the Logistic Regression model computed with respect to\n",
    "    the points (X,y) and the parameters w,b\n",
    "    \"\"\"\n",
    "    X_tilde = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "    w_tilde = np.hstack((w,b))\n",
    "    return -np.sum(y * np.log(g(w_tilde@X_tilde.T)) + (1-y) * np.log(1-g(w_tilde@X_tilde.T)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ohAlip1w0a0K"
   },
   "outputs": [],
   "source": [
    "def compute_grad(X,y,w,b):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: Nxd): the points we want to classify\n",
    "    - y (size: Nx1): the values of the classes\n",
    "    - w (size: 1xd): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    output:\n",
    "    - the gradient of the loss of the Logistic Regression model computed \n",
    "    with respect to (w,b) = w_tilde having the points (X,y) \n",
    "    \"\"\"\n",
    "    X_tilde = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "    w_tilde = np.hstack((w,b))\n",
    "    return -X_tilde.T @ (y - g(w_tilde@X_tilde.T).reshape(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "PhijbHZk0a0L"
   },
   "outputs": [],
   "source": [
    "def compute_hess(X,y,w,b):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: Nxd): the points we want to classify\n",
    "    - y (size: Nx1): the values of the classes\n",
    "    - w (size: 1xd): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    output:\n",
    "    - the hessian of the loss of the Logistic Regression model computed \n",
    "    with respect to (w,b) = w_tilde having the points (X,y) \n",
    "    \"\"\"\n",
    "    X_tilde = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "    w_tilde = np.hstack((w,b))    \n",
    "    temp = (g(w_tilde @ X_tilde.T) * (g(w_tilde @ X_tilde.T) - 1)).reshape(-1,)\n",
    "    return -X_tilde.T @ np.diag(temp) @ X_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "CB6KETwa0a0L"
   },
   "outputs": [],
   "source": [
    "def backtracking(X,y,w,b,delta,grad,alpha=0.1,beta=0.7):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: Nxd): the points we want to classify\n",
    "    - y (size: Nx1): the values of the classes\n",
    "    - w (size: 1xd): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    - delta (size n): direction of the search\n",
    "    - grad (size n): value of the gradient at point (w,b)\n",
    "    - alpha: factor of the slope of the line in the backtracking line search\n",
    "    - beta: factor of reduction of the step length\n",
    "    \n",
    "    outputs:\n",
    "    - t: the step length for the Newton step on the objective function\n",
    "    computed with backtracking line search towards delta\"\"\"\n",
    "        \n",
    "    t = 1\n",
    "    while(compute_loss(X, y, w+t*delta[:-1], b+t*delta[-1])>\n",
    "            compute_loss(X,y,w,b) + alpha*t*grad.T @ delta):\n",
    "        t = beta*t\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "c-jkPwuu0a0M"
   },
   "outputs": [],
   "source": [
    "def Newton(X, y, w0, b0, eps=pow(10,-1)):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: Nxd): the points we want to classify\n",
    "    - y (size: Nx1): the values of the classes\n",
    "    - w0 (size: 1xd): the initial weights of the affine mapping of x\n",
    "    - b0 (size: 1x1): the initial constant of the affine mapping of x\n",
    "    output:\n",
    "    - the paramer vector w_tilde_hat = (w_hat, b_hat) which maximizes the log-likelihood of \n",
    "    the sample (X,y) in the Logistic Regression model (or minimizes the loss)\n",
    "    - the cached values of the loss evaluated along training\n",
    "    \"\"\"\n",
    "    w_, b_ = w0, b0\n",
    "    grad = compute_grad(X, y, w0, b0)\n",
    "    hess = compute_hess(X, y, w0, b0)\n",
    "    \n",
    "#     inv_hess = np.linalg.inv(compute_hess(X,y,w0,b0))\n",
    "    inv_hess, _, _, _ = np.linalg.lstsq(hess, np.eye(hess.shape[0]))\n",
    "    dec_2 = grad.T@inv_hess@grad\n",
    "    Loss_hist = [compute_loss(X,y,w0,b0)]\n",
    "    while dec_2/2 > eps: # condition on the Newton decrement\n",
    "        grad = compute_grad(X,y,w_,b_)\n",
    "        hess = compute_hess(X,y,w_,b_)\n",
    "        \n",
    "#         inv_hess = np.linalg.inv(compute_hess(X,y,w_,b_))\n",
    "        inv_hess, _, _, _ = np.linalg.lstsq(hess, np.eye(hess.shape[0]))\n",
    "        dec_2 = grad.T@inv_hess@grad\n",
    "        delta = - inv_hess@grad\n",
    "        t_bt = backtracking(X, y, w_, b_, delta, grad)\n",
    "        w_ = w_ + t_bt*delta[:-1]\n",
    "        b_ = b_ + t_bt*delta[-1]\n",
    "        Loss_hist.append(compute_loss(X,y,w_,b_))\n",
    "    return w_, b_, Loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "V_kXI2MU0a0M"
   },
   "outputs": [],
   "source": [
    "def predict_LogReg(x,w,b):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - x (size 1xd): a point in R^d\n",
    "    - w (size: 1xd): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    output:\n",
    "     - the predicted class for the associated y given the\n",
    "    Logistic Regression parameters\n",
    "    \"\"\"    \n",
    "    return (w.T@x + b > 0).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "DG8epkzE0a0N"
   },
   "outputs": [],
   "source": [
    "class LogisticRegressor(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, lamb=1.):\n",
    "        \"\"\"\n",
    "        This class implements methods for fitting and predicting with a LogesticRegression for classification \n",
    "        inputs:\n",
    "        - lamb : the regularisation parameter\n",
    "        \"\"\"\n",
    "        self.lamb = lamb\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - X (size: Nxd): the points we want to classify\n",
    "        - y (size: Nx1): the values of the classes\n",
    "        outputs:\n",
    "        - the value of MLE estimation (w_hat, b_hat) in the Linear regression model\n",
    "        \"\"\"\n",
    "        w0, b0 = np.random.randn(1, 100)*0.07, np.zeros((1,1))\n",
    "        self.w_, self.b_, _ = Newton(X, y, w0, b0)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - X (size Nxd): a point in R^d\n",
    "        - w (size: 1xd): the weights of the affine mapping of x\n",
    "        - b (size: 1x1): the constant of the affine mapping of x\n",
    "        output:\n",
    "         - the predicted class for the associated y given the\n",
    "        Linear Regression parameters\n",
    "        \"\"\"    \n",
    "        return (self.w_@X.T + self.b_ > 1/2).astype(\"int\")\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - X (size Nxd): the points in R^d we want to classify\n",
    "        - y (size Nx1): the labels of the points\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.sum(y_pred == y)/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xuYAhP_70a0O",
    "outputId": "9799757c-a1f0-46b3-cb40-9a623ca6c35f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set 0: 0.62\n",
      "Accuracy on test set 0 : 0.56\n",
      "Accuracy on train set 1: 0.60\n",
      "Accuracy on test set 1 : 0.59\n",
      "Accuracy on train set 2: 0.70\n",
      "Accuracy on test set 2 : 0.66\n"
     ]
    }
   ],
   "source": [
    "dim = 100\n",
    "Nb_samples = 2000\n",
    "prop_test = 0.05\n",
    "\n",
    "all_y_eval = []\n",
    "\n",
    "np.random.seed(1)\n",
    "for name in [0, 1, 2]:\n",
    "    X = pd.read_csv(f'{data_folder}/Xtr{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
    "    y = pd.read_csv(f'{data_folder}/Ytr{name}.csv')\n",
    "    y = y[\"Bound\"].to_numpy()\n",
    "    \n",
    "    mean, std = X.mean(axis=0), X.std(axis=0)\n",
    "    X = (X - mean)/std\n",
    "\n",
    "    tr_indices = np.random.choice(Nb_samples, size=int((1-prop_test)*Nb_samples), replace=False)\n",
    "    te_indices = [idx for idx in range(Nb_samples) if idx not in tr_indices]\n",
    "\n",
    "    X_tr = X[tr_indices]\n",
    "    X_te = X[te_indices]\n",
    "    \n",
    "    y_tr = y[tr_indices]\n",
    "    y_te = y[te_indices]\n",
    "    \n",
    "    assert X_tr.shape[0] + X_te.shape[0] == X.shape[0]\n",
    "    assert y_tr.shape[0] + y_te.shape[0] == y.shape[0]\n",
    "    \n",
    "    # Fitting\n",
    "    logreg = LogisticRegressor()\n",
    "    \n",
    "    logreg.fit(X_tr, y_tr)\n",
    "\n",
    "    \n",
    "    print(f\"Accuracy on train set {name}: {logreg.score(X_tr, y_tr):.2f}\")\n",
    "    print(f\"Accuracy on test set {name} : {logreg.score(X_te, y_te):.2f}\")   \n",
    "    \n",
    "    # Prediction on the new set\n",
    "    X_eval = pd.read_csv(f'{data_folder}/Xte{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
    "    X_eval = (X_eval - mean)/std\n",
    "    y_eval = logreg.predict(X_eval)\n",
    "    all_y_eval.append(y_eval)\n",
    "    \n",
    "all_y_eval = np.hstack(all_y_eval).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "N-_1EYnY0a0O",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ids = np.arange(all_y_eval.shape[0])\n",
    "filename = \"results/submission_log_reg.csv\"\n",
    "\n",
    "# write_csv(ids, all_y_eval, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LPAM0yW0a0P"
   },
   "source": [
    "## B) Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "wZYbRtxE0a0Q"
   },
   "outputs": [],
   "source": [
    "def compute_RR_MLE(X,y,lamb):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: Nxd): the points we want to classify\n",
    "    - y (size: Nx1): the values of the classes\n",
    "    outputs:\n",
    "    - the value of MLE estimation (w_hat, b_hat) in the Linear regression model\n",
    "    \"\"\"\n",
    "    X_tilde = np.vstack((X,np.ones(X.shape[1])))\n",
    "    temp = np.linalg.inv(X_tilde@X_tilde.T + lamb*X.shape[1]*np.eye(1+X.shape[0]))@X_tilde@y.T\n",
    "    return temp[:-1], temp[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "d3923wu80a0Q"
   },
   "outputs": [],
   "source": [
    "def predict_RR(x,w,b):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - x (size 1xd): a point in R^d\n",
    "    - w (size: 1xd): the weights of the affine mapping of x\n",
    "    - b (size: 1x1): the constant of the affine mapping of x\n",
    "    output:\n",
    "     - the predicted class for the associated y given the\n",
    "    Linear Regression parameters\n",
    "    \"\"\"    \n",
    "    return (w.T@x+b>1/2).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "wux5BgTY0a0Q"
   },
   "outputs": [],
   "source": [
    "class RidgeRegressor(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, lamb=1.):\n",
    "        \"\"\"\n",
    "        This class implements methods for fitting and predicting with a RidgeRegressor used for classification \n",
    "        (by thresholding the value regressed).\n",
    "        inputs:\n",
    "        - lamb : the regularisation parameter\n",
    "        \"\"\"\n",
    "        self.lamb = lamb\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - X (size: Nxd): the points we want to classify\n",
    "        - y (size: Nx1): the values of the classes\n",
    "        outputs:\n",
    "        - the value of MLE estimation (w_hat, b_hat) in the Linear regression model\n",
    "        \"\"\"\n",
    "        X_tilde = np.hstack((X, np.ones((X.shape[0], 1))))\n",
    "        temp = np.linalg.inv(X_tilde.T @ X_tilde + self.lamb * X.shape[0] * np.eye(X_tilde.shape[1])) @ (X_tilde.T @ y)\n",
    "        self.w_ = temp[:-1]\n",
    "        self.b_ = temp[-1]\n",
    "\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - x (size Nxd): a point in R^d\n",
    "        - w (size: 1xd): the weights of the affine mapping of x\n",
    "        - b (size: 1x1): the constant of the affine mapping of x\n",
    "        output:\n",
    "         - the predicted class for the associated y given the\n",
    "        Linear Regression parameters\n",
    "        \"\"\"    \n",
    "        return (self.w_@X.T + self.b_ > 1/2).astype(\"int\")\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - X (size Nxd): the points in R^d we want to classify\n",
    "        - y (size Nx1): the labels of the points\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.sum(y_pred == y)/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zK4wdATY0a0Q",
    "outputId": "93c497fb-859c-4d42-828c-dccdf6907f6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lamb': 0.021842105263157895}\n",
      "Accuracy on train set 0: 0.65\n",
      "Accuracy on test set 0 : 0.60\n",
      "{'lamb': 0.001}\n",
      "Accuracy on train set 1: 0.64\n",
      "Accuracy on test set 1 : 0.57\n",
      "{'lamb': 0.04268421052631579}\n",
      "Accuracy on train set 2: 0.73\n",
      "Accuracy on test set 2 : 0.69\n"
     ]
    }
   ],
   "source": [
    "dim = 100\n",
    "Nb_samples = 2000\n",
    "prop_test = 0.05\n",
    "lamb = 0.1\n",
    "\n",
    "all_y_eval = []\n",
    "\n",
    "np.random.seed(1)\n",
    "for name in [0, 1, 2]:\n",
    "    # Data processing\n",
    "    X = pd.read_csv(f'{data_folder}/Xtr{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
    "    y = pd.read_csv(f'{data_folder}/Ytr{name}.csv')\n",
    "    y = y[\"Bound\"].to_numpy()\n",
    "    \n",
    "    mean, std = X.mean(axis=0), X.std(axis=0)\n",
    "    X = (X - mean)/std\n",
    "\n",
    "    tr_indices = np.random.choice(Nb_samples, size=int((1-prop_test)*Nb_samples), replace=False)\n",
    "    te_indices = [idx for idx in range(Nb_samples) if idx not in tr_indices]\n",
    "\n",
    "    X_tr = X[tr_indices]\n",
    "    X_te = X[te_indices]\n",
    "    \n",
    "    y_tr = y[tr_indices]\n",
    "    y_te = y[te_indices]\n",
    "    \n",
    "    assert X_tr.shape[0] + X_te.shape[0] == X.shape[0]\n",
    "    assert y_tr.shape[0] + y_te.shape[0] == y.shape[0]\n",
    "    \n",
    "    # Fitting the classifier\n",
    "    params = {'lamb': np.linspace(0.001, 0.1, 20)}\n",
    "    rr = GridSearchCV(RidgeRegressor(), params)\n",
    "#     rr = RidgeRegressor(lamb=lamb)\n",
    "\n",
    "    rr.fit(X_tr, y_tr)\n",
    "    \n",
    "    print(rr.best_params_)\n",
    "    \n",
    "    print(f\"Accuracy on train set {name}: {rr.score(X_tr, y_tr):.2f}\")\n",
    "    print(f\"Accuracy on test set {name} : {rr.score(X_te, y_te):.2f}\")\n",
    "    \n",
    "    # Prediction on the new set\n",
    "    X_eval = pd.read_csv(f'{data_folder}/Xte{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
    "    X_eval = (X_eval - mean)/std\n",
    "    y_eval = rr.predict(X_eval)\n",
    "    all_y_eval.append(y_eval)\n",
    "    \n",
    "all_y_eval = np.hstack(all_y_eval).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtjBOLml0a0R"
   },
   "source": [
    "# III) Kernel baselines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ie35LaC0a0R"
   },
   "source": [
    "## A) Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "1eOe1mTP0a0R"
   },
   "outputs": [],
   "source": [
    "def Gaussian_kernel(X1, X2, sig):\n",
    "    \"\"\"inputs:\n",
    "    - X1 (size N1xd): a set of points\n",
    "    - X2 (size N2xd): another one  \n",
    "    - sig (float): the std of the kernel\n",
    "    ouput:\n",
    "    - the associated (N1xN2) Gaussian kernel\n",
    "    \"\"\"\n",
    "    return np.exp(-distance_matrix(X1,X2)/(2*sig**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "9rKFOspuz7IS"
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def spectrum(x,k):\n",
    "  l = len(x)\n",
    "  spectrum_x = np.array([x[i:(i + k)] for i in range(l - k + 1)])\n",
    "  return np.array(spectrum_x)\n",
    "\n",
    "def Spectrum_kernel(X1, X2, k):\n",
    "  \"\"\"inputs:\n",
    "    - X1 (size N1xd): a set of sequences\n",
    "    - X2 (size N2xd): another one  \n",
    "    - k (int): the length of the substrings \n",
    "    ouput:\n",
    "    - the associated (N1xN2) Spectrum kernel\n",
    "    \"\"\"\n",
    "  # substrings: all possible combinations of A,T,G,C of length k\n",
    "  A_k = [''.join(s) for s in product([\"A\", \"T\", \"G\", \"C\"], repeat=k)]\n",
    "\n",
    "  # nb of occurances of the elements of A_k in the k-spectrum of X1 (resp. X2)\n",
    "  phi_spect_X1 = np.array([[np.sum(spectrum(x,k)==u) for u in A_k] for x in X1])\n",
    "  phi_spect_X2 = np.array([[np.sum(spectrum(x,k)==u) for u in A_k] for x in X2])\n",
    "  \n",
    "  return phi_spect_X1 @ phi_spect_X2.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def substring_similarity(seq_1, seq_2, k, lambd):\n",
    "    # Initialize K and B\n",
    "    K_temp = [np.ones((len(seq_1), len(seq_2)))]\n",
    "    K_temp += [np.zeros((len(seq_1), len(seq_2))) for _ in range(k)]\n",
    "\n",
    "    B_temp = [np.ones((len(seq_1), len(seq_2)))]\n",
    "    B_temp += [np.zeros((len(seq_1), len(seq_2))) for _ in range(k)]\n",
    "\n",
    "    for l in range(1, k+1):\n",
    "        # First, loop over the first sequence\n",
    "        for i_str_1 in range(len(seq_1)):\n",
    "\n",
    "            # Then, loop over the second sequence\n",
    "            for i_str_2 in range(len(seq_2)):\n",
    "                a = seq_1[i_str_1-1]\n",
    "                b = seq_2[i_str_2-1]\n",
    "                \n",
    "                # If min < l, then the matrix already has zeros in the right place : we can continue\n",
    "                if min(i_str_1, i_str_2) >= l:\n",
    "\n",
    "                    # Computation of B\n",
    "                    B_temp[l][i_str_1, i_str_2] = lambd * B_temp[l][i_str_1-1, i_str_2] \\\n",
    "                                                + lambd * B_temp[l][i_str_1, i_str_2-1] \\\n",
    "                                                - lambd ** 2 * B_temp[l][i_str_1-1, i_str_2-1] \\\n",
    "                                                + lambd ** 2 * int(a == b) * B_temp[l-1][i_str_1-1, i_str_2-1]\n",
    "\n",
    "                    # This corresponds to the sum in the computation of K -- CAN BE OPTIMIZED --\n",
    "                    K_sum_1 = 0\n",
    "                    for j_prime in range(i_str_2+1):\n",
    "                        if seq_2[j_prime] == a:\n",
    "                            K_sum_1 += B_temp[l-1][i_str_1-1, j_prime-1]\n",
    "                    \n",
    "                    # Computation of K\n",
    "                    K_temp[l][i_str_1, i_str_2] = K_temp[l][i_str_1-1, i_str_2] + lambd ** 2 * K_sum_1\n",
    "    \n",
    "    return K_temp[k][-1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substring_kernel(X1, X2, k, lambd):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    assert all([type(x) == str for x in X1]), \"not a list of strings\"\n",
    "    assert all([type(x) == str for x in X2]), \"not a list of strings\"\n",
    "    \n",
    "    K = - np.ones((len(X1), len(X2)))\n",
    "    \n",
    "    for i, seq_1 in enumerate(X1):\n",
    "        for j, seq_2 in enumerate(X2):\n",
    "            # This basically corresponds to filling the lower diagonal\n",
    "            if seq_2 in X1 and seq_1 in X2 and K[np.where(X1 == seq_2), np.where(X2 == seq_1)] != -1:\n",
    "                K[i, j] = K[np.where(X1 == seq_2), np.where(X2 == seq_1)]\n",
    "            else:\n",
    "                K[i, j] = substring_similarity(seq_1, seq_2, k, lambd)\n",
    "\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run similarity between two strings\n",
    "t0 = time.time()\n",
    "substring_similarity(\"ATGCATGATGCATG\", \"ATGCATCATGATGT\", 3, 1.)\n",
    "print(f\"{time.time() - t0:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run kernel computation between 10 strings\n",
    "X = pd.read_csv(f'{data_folder}/Xtr0.csv', sep = ',').to_numpy()\n",
    "X = X[:10,1]\n",
    "t0 = time.time()\n",
    "K = substring_kernel(X, X, k=5, lambd=1.1)\n",
    "print(f\"Time to compute K : {time.time() - t0:.2f}s\")\n",
    "# print(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBFBUDeE0a0R"
   },
   "source": [
    "## B) Algorithms\n",
    "### 1. Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "1Mg97hlq0a0R"
   },
   "outputs": [],
   "source": [
    "def compute_KRR_MLE(X, y, lamb, sig=10):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X (size: N_trxd): the points of the training set\n",
    "    - y (size: N_trx1): the values of the classes\n",
    "    outputs:\n",
    "    - the value of MLE estimation (w_hat, b_hat) in the kernel ridge regression model\n",
    "    \"\"\"\n",
    "    K = Gaussian_kernel(X, X, sig=sig)\n",
    "    alpha = np.linalg.inv(K+lamb*X.shape[1]*np.eye(X.shape[1]))@y.T\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "G8xihAC20a0R"
   },
   "outputs": [],
   "source": [
    "def predict_KRR(X_tr, X_te, alpha, sig=10):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - X_tr (size N_trxd): the points of the training set\n",
    "    - X_te (size N_texd): the points of the test set we want to classify\n",
    "    - w (size: 1xd): the weights of the affine mapping \n",
    "    - b (size: 1x1): the constant of the affine mapping\n",
    "    output:\n",
    "     - the predicted class for the associated y_te given the\n",
    "    Linear Regression parameters\n",
    "    \"\"\"    \n",
    "    K_te_tr = Gaussian_kernel(X_tr, X_te, sig=sig)\n",
    "    return 2*(alpha.T@K_te_tr>0).astype(\"int\")-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "AHIjYIWA0a0S"
   },
   "outputs": [],
   "source": [
    "class KernelRidgeRegressor(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, lamb=1., sigma=1., kernel='gaussian'):\n",
    "        \"\"\"\n",
    "        This class implements methods for fitting and predicting with a KernelRidgeRegressor used for classification \n",
    "        (by thresholding the value regressed). Any kernel can be used. \n",
    "        inputs:\n",
    "        - lamb : the regularisation parameter \n",
    "        - sigma : the parameter of the Gaussian kernel (if Gaussian kernel selected)\n",
    "        - kernel : the kernel we consider\n",
    "        \"\"\"\n",
    "        self.lamb = lamb\n",
    "        self.sigma = sigma\n",
    "        self.kernel = kernel\n",
    "        if self.kernel == 'gaussian':\n",
    "            self.kernel_ = partial(Gaussian_kernel, sig=sigma)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Kernel {self.kernel} is not implemented yet\")\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - X (size: N_trxd): the points of the training set\n",
    "        - y (size: N_trx1): the values of the classes\n",
    "        \"\"\"\n",
    "        # We keep values of training in memory for prediction\n",
    "        self.X_tr_ = np.copy(X)\n",
    "        K = self.kernel_(X, X, sig=self.sigma)\n",
    "        self.alpha_ = np.linalg.inv(K+self.lamb*X.shape[0]*np.eye(X.shape[0]))@y\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - X (size N_texd): the points in R^d we want to classify\n",
    "        output:\n",
    "         - the predicted class for the associated y given the\n",
    "        Linear Regression parameters\n",
    "        \"\"\"\n",
    "        K_tr_te = self.kernel_(self.X_tr_, X, sig=self.sigma)\n",
    "        \n",
    "        return 2 * (self.alpha_.T@K_tr_te > 0).reshape(-1, ).astype(\"int\") - 1\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - X (size N_texd): the points in R^d we want to classify\n",
    "        - y (size N_tex1): the labels of the points\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        return np.sum(y_pred == y)/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kgsnnyjd0a0S",
    "outputId": "a442b499-687a-45c1-e093-5c024d87d119",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': 'gaussian', 'lamb': 0.1, 'sigma': 0.5789473684210527}\n",
      "Accuracy on train set 0: 1.00\n",
      "Accuracy on test set 0 : 0.57\n",
      "{'kernel': 'gaussian', 'lamb': 0.1, 'sigma': 0.6578947368421053}\n",
      "Accuracy on train set 1: 1.00\n",
      "Accuracy on test set 1 : 0.59\n",
      "{'kernel': 'gaussian', 'lamb': 0.1, 'sigma': 0.5}\n",
      "Accuracy on train set 2: 1.00\n",
      "Accuracy on test set 2 : 0.67\n"
     ]
    }
   ],
   "source": [
    "dim = 100\n",
    "Nb_samples = 2000\n",
    "prop_test = 0.2\n",
    "lamb = 0.5\n",
    "sigma = 1.2\n",
    "\n",
    "all_y_eval = []\n",
    "\n",
    "np.random.seed(1)\n",
    "for name in [0, 1, 2]:\n",
    "    # Data Processing\n",
    "    X = pd.read_csv(f'{data_folder}/Xtr{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
    "    y = pd.read_csv(f'{data_folder}/Ytr{name}.csv')\n",
    "    y = y[\"Bound\"].to_numpy()\n",
    "    y[y==0] = -1\n",
    "    \n",
    "    mean, std = X.mean(axis=0), X.std(axis=0)\n",
    "    X = (X - mean)/std\n",
    "\n",
    "    tr_indices = np.random.choice(Nb_samples, size=int((1-prop_test)*Nb_samples), replace=False)\n",
    "    te_indices = [idx for idx in range(Nb_samples) if idx not in tr_indices]\n",
    "\n",
    "    X_tr = X[tr_indices]\n",
    "    X_te = X[te_indices]\n",
    "    \n",
    "    y_tr = y[tr_indices]\n",
    "    y_te = y[te_indices]\n",
    "    \n",
    "    assert X_tr.shape[0] + X_te.shape[0] == X.shape[0]\n",
    "    assert y_tr.shape[0] + y_te.shape[0] == y.shape[0]\n",
    "    \n",
    "    # Fitting\n",
    "    params = {'lamb': np.linspace(0.1, 2, 2), 'sigma': np.linspace(0.5, 2, 20), 'kernel': ['gaussian']}\n",
    "    krr = GridSearchCV(KernelRidgeRegressor(), params)\n",
    "#     krr = KernelRidgeRegressor()\n",
    "    \n",
    "    krr.fit(X_tr,y_tr)\n",
    "    \n",
    "    print(krr.best_params_)\n",
    "    \n",
    "    print(f\"Accuracy on train set {name}: {krr.score(X_tr, y_tr):.2f}\")\n",
    "    print(f\"Accuracy on test set {name} : {krr.score(X_te, y_te):.2f}\")\n",
    "    \n",
    "    # Prediction on the new set\n",
    "    X_eval = pd.read_csv(f'{data_folder}/Xte{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
    "    X_eval = (X_eval - mean)/std\n",
    "    y_eval = krr.predict(X_eval)\n",
    "    all_y_eval.append(y_eval)\n",
    "    \n",
    "all_y_eval = np.hstack(all_y_eval).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9m6sPN00a0S"
   },
   "source": [
    "### 2. Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "dhSUDIUp0a0S"
   },
   "outputs": [],
   "source": [
    "class KernelSVM(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, lamb=1., sigma=1., k = 3, kernel='gaussian'):\n",
    "        \"\"\"\n",
    "        This class implements methods for fitting and predicting with a KernelRidgeRegressor used for classification \n",
    "        (by thresholding the value regressed). Any kernel can be used. \n",
    "        inputs:\n",
    "        - lamb : the regularisation parameter \n",
    "        - sigma : the parameter of the Gaussian kernel (if Gaussian kernel selected)\n",
    "        - kernel : the kernel we consider\n",
    "        \"\"\"\n",
    "        self.lamb = lamb\n",
    "        self.sigma = sigma\n",
    "        self.k = k\n",
    "        self.kernel = kernel\n",
    "        if self.kernel == 'gaussian':\n",
    "            self.kernel_ = partial(Gaussian_kernel, sig=sigma)\n",
    "        elif self.kernel == 'spectrum':\n",
    "            self.kernel_ = partial(Spectrum_kernel, k=k)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Kernel {self.kernel} is not implemented yet\")\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - X (size: N_trxd): the points of the training set\n",
    "        - y (size: N_trx1): the values of the classes\n",
    "        \"\"\"\n",
    "        # We keep values of training in memory for prediction\n",
    "        N_tr = X.shape[0]\n",
    "        self.X_tr_ = np.copy(X)\n",
    "\n",
    "        if self.kernel == 'gaussian':\n",
    "          K = self.kernel_(X, X, sig=self.sigma)\n",
    "        elif self.kernel == 'spectrum':\n",
    "          K = self.kernel_(X, X, k=self.k)\n",
    "        # Define QP and solve it with cvxpy\n",
    "        alpha = cp.Variable(N_tr)\n",
    "        objective = cp.Maximize(2*alpha.T@y - cp.quad_form(alpha, K))\n",
    "        constraints = [0 <= cp.multiply(y,alpha), cp.multiply(y,alpha) <= 1/(2*self.lamb*N_tr)]\n",
    "        prob = cp.Problem(objective, constraints)\n",
    "\n",
    "        # The optimal objective value is returned by `prob.solve()`.\n",
    "        result = prob.solve()\n",
    "        # The optimal value for x is stored in `x.value`.\n",
    "        self.alpha_ = alpha.value\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - X (size N_texd): the points in R^d we want to classify\n",
    "        output:\n",
    "         - the predicted class for the associated y given the\n",
    "        Linear Regression parameters\n",
    "        \"\"\"\n",
    "        if self.kernel == 'gaussian':\n",
    "          K_tr_te = self.kernel_(self.X_tr_, X, sig=self.sigma)\n",
    "        elif self.kernel == 'spectrum':\n",
    "          K_tr_te = self.kernel_(self.X_tr_, X, k=self.k)\n",
    "        \n",
    "        return 2 * (self.alpha_.T@K_tr_te > 0).reshape(-1, ).astype(\"int\") - 1\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - X (size N_texd): the points in R^d we want to classify\n",
    "        - y (size N_tex1): the labels of the points\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        return np.sum(y_pred == y)/y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTsajMKQSp2_"
   },
   "source": [
    "#### Gaussian Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNl7NJXm0a0S"
   },
   "outputs": [],
   "source": [
    "dim = 100\n",
    "Nb_samples = 2000\n",
    "prop_test = 0.2\n",
    "lamb = 0.5\n",
    "sigma = 1.2\n",
    "\n",
    "all_y_eval = []\n",
    "\n",
    "np.random.seed(1)\n",
    "for name in [0, 1, 2]:\n",
    "    # Data Processing\n",
    "    X = pd.read_csv(f'{data_folder}/Xtr{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
    "    y = pd.read_csv(f'{data_folder}/Ytr{name}.csv')\n",
    "    y = y[\"Bound\"].to_numpy()\n",
    "    y[y==0] = -1\n",
    "    \n",
    "    mean, std = X.mean(axis=0), X.std(axis=0)\n",
    "    X = (X - mean)/std\n",
    "\n",
    "    tr_indices = np.random.choice(Nb_samples, size=int((1-prop_test)*Nb_samples), replace=False)\n",
    "    te_indices = [idx for idx in range(Nb_samples) if idx not in tr_indices]\n",
    "\n",
    "    X_tr = X[tr_indices]\n",
    "    X_te = X[te_indices]\n",
    "    \n",
    "    y_tr = y[tr_indices]\n",
    "    y_te = y[te_indices]\n",
    "    \n",
    "    assert X_tr.shape[0] + X_te.shape[0] == X.shape[0]\n",
    "    assert y_tr.shape[0] + y_te.shape[0] == y.shape[0]\n",
    "    \n",
    "    # Fitting\n",
    "    params = {'lamb': np.logspace(-10., -7., 4), 'sigma': np.logspace(-1., 2., 4), 'kernel': ['gaussian']}\n",
    "    ksvm = GridSearchCV(KernelSVM(), params)\n",
    "#     krr = KernelRidgeRegressor()\n",
    "    \n",
    "    ksvm.fit(X_tr,y_tr)\n",
    "    \n",
    "    print(ksvm.best_params_)\n",
    "    \n",
    "    print(f\"Accuracy on train set {name}: {ksvm.score(X_tr, y_tr):.2f}\")\n",
    "    print(f\"Accuracy on test set {name} : {ksvm.score(X_te, y_te):.2f}\")\n",
    "    \n",
    "    # Prediction on the new set\n",
    "    X_eval = pd.read_csv(f'{data_folder}/Xte{name}_mat100.csv', sep = ' ', index_col=False, header=None).to_numpy()\n",
    "    X_eval = (X_eval - mean)/std\n",
    "    y_eval = ksvm.predict(X_eval)\n",
    "    all_y_eval.append(y_eval)\n",
    "    \n",
    "all_y_eval = np.hstack(all_y_eval).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gPd2jimSvGR"
   },
   "source": [
    "#### Spectrum Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "hjpfylDEImWk",
    "outputId": "15104b61-5bd3-443c-e2cf-45382846a0ab"
   },
   "outputs": [
    {
     "ename": "DCPError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDCPError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-d1582ef685ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mksvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKernelSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlamb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'spectrum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mksvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mksvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-117-0dd8df7deb85>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# The optimal objective value is returned by `prob.solve()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m# The optimal value for x is stored in `x.value`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cvxpy/problems/problem.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0msolve_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msolve_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cvxpy/problems/problem.py\u001b[0m in \u001b[0;36m_solve\u001b[0;34m(self, solver, warm_start, verbose, parallel, gp, qcp, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m                     solver, warm_start, verbose, **kwargs)\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_chains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m         data, solving_inverse_data = self._solving_chain.apply(\n\u001b[1;32m    570\u001b[0m             self._intermediate_problem)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cvxpy/problems/problem.py\u001b[0m in \u001b[0;36m_construct_chains\u001b[0;34m(self, solver, gp)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     def _solve(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cvxpy/problems/problem.py\u001b[0m in \u001b[0;36m_construct_chains\u001b[0;34m(self, solver, gp)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intermediate_chain\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m                     \u001b[0mconstruct_intermediate_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_solvers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intermediate_problem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intermediate_inverse_data\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intermediate_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cvxpy/reductions/solvers/intermediate_chain.py\u001b[0m in \u001b[0;36mconstruct_intermediate_chain\u001b[0;34m(problem, candidates, gp)\u001b[0m\n\u001b[1;32m     94\u001b[0m             append += (\"\\nHowever, the problem does follow DQCP rules. \"\n\u001b[1;32m     95\u001b[0m                        \"Consider calling solve() with `qcp=True`.\")\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mDCPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Problem does not follow DCP rules. Specifically:\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mgp\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dgp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDCPError\u001b[0m: Problem does not follow DCP rules. Specifically:\nThe objective is not DCP. Its following subexpressions are not:\nQuadForm(var14131, [[329. 166. 155. ... 149. 182. 115.]\n [166. 299. 138. ... 181. 198. 261.]\n [155. 138. 349. ... 146. 148. 173.]\n ...\n [149. 181. 146. ... 279. 162. 180.]\n [182. 198. 148. ... 162. 301. 174.]\n [115. 261. 173. ... 180. 174. 425.]])"
     ]
    }
   ],
   "source": [
    "## Kernel SVM with Spectrum kernel\n",
    "\n",
    "dim = 100\n",
    "Nb_samples = 2000\n",
    "prop_test = 0.2\n",
    "lamb = 0.5\n",
    "sigma = 1.2\n",
    "k = 3\n",
    "\n",
    "all_y_eval = []\n",
    "\n",
    "np.random.seed(1)\n",
    "for name in [0, 1, 2]:\n",
    "    # Data Processing\n",
    "    df = pd.read_csv(f'{data_folder}/Xtr{name}.csv')\n",
    "    X = np.array(df['seq'])\n",
    "    y = pd.read_csv(f'{data_folder}/Ytr{name}.csv')\n",
    "    y = y[\"Bound\"].to_numpy()\n",
    "    y[y==0] = -1\n",
    "    \n",
    "\n",
    "    tr_indices = np.random.choice(Nb_samples, size=int((1-prop_test)*Nb_samples), replace=False)\n",
    "    te_indices = [idx for idx in range(Nb_samples) if idx not in tr_indices]\n",
    "\n",
    "    X_tr = X[tr_indices]\n",
    "    X_te = X[te_indices]\n",
    "    \n",
    "    y_tr = y[tr_indices]\n",
    "    y_te = y[te_indices]\n",
    "    \n",
    "    assert X_tr.shape[0] + X_te.shape[0] == X.shape[0]\n",
    "    assert y_tr.shape[0] + y_te.shape[0] == y.shape[0]\n",
    "    \n",
    "    # Fitting\n",
    "    # params = {'lamb': np.logspace(-10., -7., 4), 'k': np.array([3,4,5,6]), 'kernel': ['spectrum']}\n",
    "    # ksvm = GridSearchCV(KernelSVM(), params)\n",
    "#     krr = KernelRidgeRegressor()\n",
    "    ksvm = KernelSVM(lamb = lamb, k=k, kernel='spectrum')\n",
    "    \n",
    "    ksvm.fit(X_tr,y_tr)\n",
    "    \n",
    "    print(ksvm.best_params_)\n",
    "    \n",
    "    print(f\"Accuracy on train set {name}: {ksvm.score(X_tr, y_tr):.2f}\")\n",
    "    print(f\"Accuracy on test set {name} : {ksvm.score(X_te, y_te):.2f}\")\n",
    "    \n",
    "    # Prediction on the new set\n",
    "    X_eval = np.array(pd.read_csv(f'{data_folder}/Xte{name}.csv')['seq'])\n",
    "    y_eval = ksvm.predict(X_eval)\n",
    "    all_y_eval.append(y_eval)\n",
    "    \n",
    "all_y_eval = np.hstack(all_y_eval).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y8mljyosSk1h",
    "outputId": "61f25f56-d176-4aeb-823f-769b2ef71c9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.67373760e+05,  4.32026053e+04,  3.23499785e+04, ...,\n",
       "        7.12260801e-14, -1.66362557e-14,  2.67708609e-14])"
      ]
     },
     "execution_count": 122,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "Xtr_0 = pd.read_csv('data/Xtr0.csv')\n",
    "X_train_0 = np.array(Xtr_0['seq'])\n",
    "\n",
    "SPK = Spectrum_kernel(X_train_0,X_train_0,k=3)\n",
    "np.real(np.linalg.eig(SPK)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aARgH1w10a0T"
   },
   "outputs": [],
   "source": [
    "ids = np.arange(all_y_eval.shape[0])\n",
    "filename = \"results/submission_gaussian_svm.csv\"\n",
    "\n",
    "# write_csv(ids, all_y_eval, filename)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "KM_challenge.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
